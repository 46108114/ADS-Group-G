{"cells":[{"cell_type":"markdown","metadata":{"id":"8rSqEFCgz4In"},"source":["# ADS Group G"]},{"cell_type":"markdown","metadata":{"id":"o1bFR9wgz6Jd"},"source":["## Setting up COCO API Environment\n","\n","This section compiles all the python and Linux based commands required in order to perform the Evaluation and Visualization scripts provided by the researchers on the COCO 2017 Validation Dataset and our New Datasets."]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Import required libraries\n","import os\n","import pandas as pd\n","import json"]},{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":41100,"status":"ok","timestamp":1636111122655,"user":{"displayName":"David Miguel Barles Dela Cruz","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"16734584818421110252"},"user_tz":-660},"id":"HZ_2MJQ6oOeF","outputId":"86a4e6dc-32c4-42b6-8423-cd171d707c24"},"outputs":[{"name":"stdout","output_type":"stream","text":["Mounted at /content/gdrive\n"]}],"source":["# Only exectute if using Google Colab\n","# Google Colab Mount\n","\n","from google.colab import drive\n","\n","drive.mount('/content/gdrive')"]},{"cell_type":"code","execution_count":2,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":507,"status":"ok","timestamp":1636111123158,"user":{"displayName":"David Miguel Barles Dela Cruz","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"16734584818421110252"},"user_tz":-660},"id":"tURYwoR1oWKH","outputId":"b4b110ee-5a4d-494e-f90d-c564edbd3a12"},"outputs":[{"name":"stdout","output_type":"stream","text":["cocoapi      experiments  log\t\t\tREADME.md\n","_config.yml  figures\t  models\t\trequirements.txt\n","data\t     lib\t  output\t\ttools\n","demo\t     LICENSE\t  pose_coco_models.zip\tvisualization\n"]}],"source":["# Only exectute if using Google Colab\n","# os.chdir('/content/gdrive/MyDrive/ColabNotebooks/ADS_Project/deep-high-resolution-net.pytorch-master') # Google Drive Path to Code Directort\n","\n","!ls"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":17383,"status":"ok","timestamp":1636100129462,"user":{"displayName":"David Miguel Barles Dela Cruz","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"16734584818421110252"},"user_tz":-660},"id":"-qJLjp_ro2pW","outputId":"068704c5-d55b-4a1d-ebab-66dca0a2314a"},"outputs":[{"name":"stdout","output_type":"stream","text":["Collecting EasyDict==1.7\n","  Downloading easydict-1.7.tar.gz (6.2 kB)\n","Requirement already satisfied: opencv-python in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 2)) (4.1.2.30)\n","Collecting shapely==1.6.4\n","  Downloading Shapely-1.6.4.tar.gz (224 kB)\n","\u001b[K     |████████████████████████████████| 224 kB 5.3 MB/s \n","\u001b[?25hRequirement already satisfied: Cython in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 4)) (0.29.24)\n","Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 5)) (1.4.1)\n","Requirement already satisfied: pandas in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 6)) (1.1.5)\n","Requirement already satisfied: pyyaml in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 7)) (3.13)\n","Collecting json_tricks\n","  Downloading json_tricks-3.15.5-py2.py3-none-any.whl (26 kB)\n","Requirement already satisfied: scikit-image in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 9)) (0.16.2)\n","Collecting yacs>=0.1.5\n","  Downloading yacs-0.1.8-py3-none-any.whl (14 kB)\n","Collecting tensorboardX==1.6\n","  Downloading tensorboardX-1.6-py2.py3-none-any.whl (129 kB)\n","\u001b[K     |████████████████████████████████| 129 kB 49.9 MB/s \n","\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from tensorboardX==1.6->-r requirements.txt (line 11)) (1.19.5)\n","Requirement already satisfied: protobuf>=3.2.0 in /usr/local/lib/python3.7/dist-packages (from tensorboardX==1.6->-r requirements.txt (line 11)) (3.17.3)\n","Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from tensorboardX==1.6->-r requirements.txt (line 11)) (1.15.0)\n","Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.7/dist-packages (from pandas->-r requirements.txt (line 6)) (2018.9)\n","Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas->-r requirements.txt (line 6)) (2.8.2)\n","Requirement already satisfied: networkx>=2.0 in /usr/local/lib/python3.7/dist-packages (from scikit-image->-r requirements.txt (line 9)) (2.6.3)\n","Requirement already satisfied: PyWavelets>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from scikit-image->-r requirements.txt (line 9)) (1.1.1)\n","Requirement already satisfied: matplotlib!=3.0.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from scikit-image->-r requirements.txt (line 9)) (3.2.2)\n","Requirement already satisfied: pillow>=4.3.0 in /usr/local/lib/python3.7/dist-packages (from scikit-image->-r requirements.txt (line 9)) (7.1.2)\n","Requirement already satisfied: imageio>=2.3.0 in /usr/local/lib/python3.7/dist-packages (from scikit-image->-r requirements.txt (line 9)) (2.4.1)\n","Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib!=3.0.0,>=2.0.0->scikit-image->-r requirements.txt (line 9)) (0.10.0)\n","Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib!=3.0.0,>=2.0.0->scikit-image->-r requirements.txt (line 9)) (2.4.7)\n","Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib!=3.0.0,>=2.0.0->scikit-image->-r requirements.txt (line 9)) (1.3.2)\n","Building wheels for collected packages: EasyDict, shapely\n","  Building wheel for EasyDict (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for EasyDict: filename=easydict-1.7-py3-none-any.whl size=6120 sha256=87109eccf5ae3e99dd8c6f4c0afc2543ecbe1dbcffc9f01d46ff4f010622a555\n","  Stored in directory: /root/.cache/pip/wheels/1d/be/cf/14f66f4c4cea5ebf212d9ce559c8a0d8f4882ecc7f59a6710c\n","  Building wheel for shapely (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for shapely: filename=Shapely-1.6.4-cp37-cp37m-linux_x86_64.whl size=635717 sha256=a17d1100478a01f23e09d5b4bd4184f8f1c28ba76b1389fb5534e23982291397\n","  Stored in directory: /root/.cache/pip/wheels/e3/8a/b6/48c41d66a2abc4fe5cc0916fda5e0f7121f1e5266f71747256\n","Successfully built EasyDict shapely\n","Installing collected packages: yacs, tensorboardX, shapely, json-tricks, EasyDict\n","  Attempting uninstall: shapely\n","    Found existing installation: Shapely 1.7.1\n","    Uninstalling Shapely-1.7.1:\n","      Successfully uninstalled Shapely-1.7.1\n","  Attempting uninstall: EasyDict\n","    Found existing installation: easydict 1.9\n","    Uninstalling easydict-1.9:\n","      Successfully uninstalled easydict-1.9\n","\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n","albumentations 0.1.12 requires imgaug<0.2.7,>=0.2.5, but you have imgaug 0.2.9 which is incompatible.\u001b[0m\n","Successfully installed EasyDict-1.7 json-tricks-3.15.5 shapely-1.6.4 tensorboardX-1.6 yacs-0.1.8\n"]}],"source":["! pip install -r requirements.txt"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":1181,"status":"ok","timestamp":1636100130639,"user":{"displayName":"David Miguel Barles Dela Cruz","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"16734584818421110252"},"user_tz":-660},"id":"H6ewpqPZy0Iv","outputId":"f27acc39-95fd-4dea-d919-74b283bca7f6"},"outputs":[{"name":"stdout","output_type":"stream","text":["cd nms; python setup_linux.py build_ext --inplace; rm -rf build; cd ../../\n","running build_ext\n","skipping 'cpu_nms.c' Cython extension (up-to-date)\n","skipping 'gpu_nms.cpp' Cython extension (up-to-date)\n"]}],"source":["os.chdir('lib/')\n","!make\n","os.chdir('../')"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":10636,"status":"ok","timestamp":1636100141272,"user":{"displayName":"David Miguel Barles Dela Cruz","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"16734584818421110252"},"user_tz":-660},"id":"OpV_SzGwy-n8","outputId":"79a269df-74a9-4cb3-ba16-c240e8854b2b"},"outputs":[{"name":"stdout","output_type":"stream","text":["# install pycocotools to the Python site-packages\n","python setup.py build_ext install\n","running build_ext\n","skipping 'pycocotools/_mask.c' Cython extension (up-to-date)\n","building 'pycocotools._mask' extension\n","creating build\n","creating build/common\n","creating build/temp.linux-x86_64-3.7\n","creating build/temp.linux-x86_64-3.7/pycocotools\n","x86_64-linux-gnu-gcc -pthread -Wno-unused-result -Wsign-compare -DNDEBUG -g -fwrapv -O2 -Wall -g -fdebug-prefix-map=/build/python3.7-Y7dWVB/python3.7-3.7.12=. -fstack-protector-strong -Wformat -Werror=format-security -g -fdebug-prefix-map=/build/python3.7-Y7dWVB/python3.7-3.7.12=. -fstack-protector-strong -Wformat -Werror=format-security -Wdate-time -D_FORTIFY_SOURCE=2 -fPIC -I/usr/local/lib/python3.7/dist-packages/numpy/core/include -I../common -I/usr/include/python3.7m -c ../common/maskApi.c -o build/temp.linux-x86_64-3.7/../common/maskApi.o -Wno-cpp -Wno-unused-function -std=c99\n","\u001b[01m\u001b[K../common/maskApi.c:\u001b[m\u001b[K In function ‘\u001b[01m\u001b[KrleDecode\u001b[m\u001b[K’:\n","\u001b[01m\u001b[K../common/maskApi.c:46:7:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[Kthis ‘\u001b[01m\u001b[Kfor\u001b[m\u001b[K’ clause does not guard... [\u001b[01;35m\u001b[K-Wmisleading-indentation\u001b[m\u001b[K]\n","       \u001b[01;35m\u001b[Kfor\u001b[m\u001b[K( k=0; k<R[i].cnts[j]; k++ ) *(M++)=v; v=!v; }}\n","       \u001b[01;35m\u001b[K^~~\u001b[m\u001b[K\n","\u001b[01m\u001b[K../common/maskApi.c:46:49:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[K...this statement, but the latter is misleadingly indented as if it were guarded by the ‘\u001b[01m\u001b[Kfor\u001b[m\u001b[K’\n","       for( k=0; k<R[i].cnts[j]; k++ ) *(M++)=v; \u001b[01;36m\u001b[Kv\u001b[m\u001b[K=!v; }}\n","                                                 \u001b[01;36m\u001b[K^\u001b[m\u001b[K\n","\u001b[01m\u001b[K../common/maskApi.c:\u001b[m\u001b[K In function ‘\u001b[01m\u001b[KrleFrPoly\u001b[m\u001b[K’:\n","\u001b[01m\u001b[K../common/maskApi.c:166:3:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[Kthis ‘\u001b[01m\u001b[Kfor\u001b[m\u001b[K’ clause does not guard... [\u001b[01;35m\u001b[K-Wmisleading-indentation\u001b[m\u001b[K]\n","   \u001b[01;35m\u001b[Kfor\u001b[m\u001b[K(j=0; j<k; j++) x[j]=(int)(scale*xy[j*2+0]+.5); x[k]=x[0];\n","   \u001b[01;35m\u001b[K^~~\u001b[m\u001b[K\n","\u001b[01m\u001b[K../common/maskApi.c:166:54:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[K...this statement, but the latter is misleadingly indented as if it were guarded by the ‘\u001b[01m\u001b[Kfor\u001b[m\u001b[K’\n","   for(j=0; j<k; j++) x[j]=(int)(scale*xy[j*2+0]+.5); \u001b[01;36m\u001b[Kx\u001b[m\u001b[K[k]=x[0];\n","                                                      \u001b[01;36m\u001b[K^\u001b[m\u001b[K\n","\u001b[01m\u001b[K../common/maskApi.c:167:3:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[Kthis ‘\u001b[01m\u001b[Kfor\u001b[m\u001b[K’ clause does not guard... [\u001b[01;35m\u001b[K-Wmisleading-indentation\u001b[m\u001b[K]\n","   \u001b[01;35m\u001b[Kfor\u001b[m\u001b[K(j=0; j<k; j++) y[j]=(int)(scale*xy[j*2+1]+.5); y[k]=y[0];\n","   \u001b[01;35m\u001b[K^~~\u001b[m\u001b[K\n","\u001b[01m\u001b[K../common/maskApi.c:167:54:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[K...this statement, but the latter is misleadingly indented as if it were guarded by the ‘\u001b[01m\u001b[Kfor\u001b[m\u001b[K’\n","   for(j=0; j<k; j++) y[j]=(int)(scale*xy[j*2+1]+.5); \u001b[01;36m\u001b[Ky\u001b[m\u001b[K[k]=y[0];\n","                                                      \u001b[01;36m\u001b[K^\u001b[m\u001b[K\n","\u001b[01m\u001b[K../common/maskApi.c:\u001b[m\u001b[K In function ‘\u001b[01m\u001b[KrleToString\u001b[m\u001b[K’:\n","\u001b[01m\u001b[K../common/maskApi.c:212:7:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[Kthis ‘\u001b[01m\u001b[Kif\u001b[m\u001b[K’ clause does not guard... [\u001b[01;35m\u001b[K-Wmisleading-indentation\u001b[m\u001b[K]\n","       \u001b[01;35m\u001b[Kif\u001b[m\u001b[K(more) c |= 0x20; c+=48; s[p++]=c;\n","       \u001b[01;35m\u001b[K^~\u001b[m\u001b[K\n","\u001b[01m\u001b[K../common/maskApi.c:212:27:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[K...this statement, but the latter is misleadingly indented as if it were guarded by the ‘\u001b[01m\u001b[Kif\u001b[m\u001b[K’\n","       if(more) c |= 0x20; \u001b[01;36m\u001b[Kc\u001b[m\u001b[K+=48; s[p++]=c;\n","                           \u001b[01;36m\u001b[K^\u001b[m\u001b[K\n","\u001b[01m\u001b[K../common/maskApi.c:\u001b[m\u001b[K In function ‘\u001b[01m\u001b[KrleFrString\u001b[m\u001b[K’:\n","\u001b[01m\u001b[K../common/maskApi.c:220:3:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[Kthis ‘\u001b[01m\u001b[Kwhile\u001b[m\u001b[K’ clause does not guard... [\u001b[01;35m\u001b[K-Wmisleading-indentation\u001b[m\u001b[K]\n","   \u001b[01;35m\u001b[Kwhile\u001b[m\u001b[K( s[m] ) m++; cnts=malloc(sizeof(uint)*m); m=0;\n","   \u001b[01;35m\u001b[K^~~~~\u001b[m\u001b[K\n","\u001b[01m\u001b[K../common/maskApi.c:220:22:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[K...this statement, but the latter is misleadingly indented as if it were guarded by the ‘\u001b[01m\u001b[Kwhile\u001b[m\u001b[K’\n","   while( s[m] ) m++; \u001b[01;36m\u001b[Kcnts\u001b[m\u001b[K=malloc(sizeof(uint)*m); m=0;\n","                      \u001b[01;36m\u001b[K^~~~\u001b[m\u001b[K\n","\u001b[01m\u001b[K../common/maskApi.c:228:5:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[Kthis ‘\u001b[01m\u001b[Kif\u001b[m\u001b[K’ clause does not guard... [\u001b[01;35m\u001b[K-Wmisleading-indentation\u001b[m\u001b[K]\n","     \u001b[01;35m\u001b[Kif\u001b[m\u001b[K(m>2) x+=(long) cnts[m-2]; cnts[m++]=(uint) x;\n","     \u001b[01;35m\u001b[K^~\u001b[m\u001b[K\n","\u001b[01m\u001b[K../common/maskApi.c:228:34:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[K...this statement, but the latter is misleadingly indented as if it were guarded by the ‘\u001b[01m\u001b[Kif\u001b[m\u001b[K’\n","     if(m>2) x+=(long) cnts[m-2]; \u001b[01;36m\u001b[Kcnts\u001b[m\u001b[K[m++]=(uint) x;\n","                                  \u001b[01;36m\u001b[K^~~~\u001b[m\u001b[K\n","\u001b[01m\u001b[K../common/maskApi.c:\u001b[m\u001b[K In function ‘\u001b[01m\u001b[KrleToBbox\u001b[m\u001b[K’:\n","\u001b[01m\u001b[K../common/maskApi.c:141:31:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[K‘\u001b[01m\u001b[Kxp\u001b[m\u001b[K’ may be used uninitialized in this function [\u001b[01;35m\u001b[K-Wmaybe-uninitialized\u001b[m\u001b[K]\n","       if(j%2==0) xp=x; else if\u001b[01;35m\u001b[K(\u001b[m\u001b[Kxp<x) { ys=0; ye=h-1; }\n","                               \u001b[01;35m\u001b[K^\u001b[m\u001b[K\n","x86_64-linux-gnu-gcc -pthread -Wno-unused-result -Wsign-compare -DNDEBUG -g -fwrapv -O2 -Wall -g -fdebug-prefix-map=/build/python3.7-Y7dWVB/python3.7-3.7.12=. -fstack-protector-strong -Wformat -Werror=format-security -g -fdebug-prefix-map=/build/python3.7-Y7dWVB/python3.7-3.7.12=. -fstack-protector-strong -Wformat -Werror=format-security -Wdate-time -D_FORTIFY_SOURCE=2 -fPIC -I/usr/local/lib/python3.7/dist-packages/numpy/core/include -I../common -I/usr/include/python3.7m -c pycocotools/_mask.c -o build/temp.linux-x86_64-3.7/pycocotools/_mask.o -Wno-cpp -Wno-unused-function -std=c99\n","creating build/lib.linux-x86_64-3.7\n","creating build/lib.linux-x86_64-3.7/pycocotools\n","x86_64-linux-gnu-gcc -pthread -shared -Wl,-O1 -Wl,-Bsymbolic-functions -Wl,-Bsymbolic-functions -Wl,-z,relro -Wl,-Bsymbolic-functions -Wl,-z,relro -g -fdebug-prefix-map=/build/python3.7-Y7dWVB/python3.7-3.7.12=. -fstack-protector-strong -Wformat -Werror=format-security -Wdate-time -D_FORTIFY_SOURCE=2 build/temp.linux-x86_64-3.7/../common/maskApi.o build/temp.linux-x86_64-3.7/pycocotools/_mask.o -o build/lib.linux-x86_64-3.7/pycocotools/_mask.cpython-37m-x86_64-linux-gnu.so\n","running install\n","running bdist_egg\n","running egg_info\n","writing pycocotools.egg-info/PKG-INFO\n","writing dependency_links to pycocotools.egg-info/dependency_links.txt\n","writing requirements to pycocotools.egg-info/requires.txt\n","writing top-level names to pycocotools.egg-info/top_level.txt\n","writing manifest file 'pycocotools.egg-info/SOURCES.txt'\n","installing library code to build/bdist.linux-x86_64/egg\n","running install_lib\n","running build_py\n","copying pycocotools/__init__.py -> build/lib.linux-x86_64-3.7/pycocotools\n","copying pycocotools/coco.py -> build/lib.linux-x86_64-3.7/pycocotools\n","copying pycocotools/cocoeval.py -> build/lib.linux-x86_64-3.7/pycocotools\n","copying pycocotools/mask.py -> build/lib.linux-x86_64-3.7/pycocotools\n","creating build/bdist.linux-x86_64\n","creating build/bdist.linux-x86_64/egg\n","creating build/bdist.linux-x86_64/egg/pycocotools\n","copying build/lib.linux-x86_64-3.7/pycocotools/_mask.cpython-37m-x86_64-linux-gnu.so -> build/bdist.linux-x86_64/egg/pycocotools\n","copying build/lib.linux-x86_64-3.7/pycocotools/__init__.py -> build/bdist.linux-x86_64/egg/pycocotools\n","copying build/lib.linux-x86_64-3.7/pycocotools/coco.py -> build/bdist.linux-x86_64/egg/pycocotools\n","copying build/lib.linux-x86_64-3.7/pycocotools/cocoeval.py -> build/bdist.linux-x86_64/egg/pycocotools\n","copying build/lib.linux-x86_64-3.7/pycocotools/mask.py -> build/bdist.linux-x86_64/egg/pycocotools\n","byte-compiling build/bdist.linux-x86_64/egg/pycocotools/__init__.py to __init__.cpython-37.pyc\n","byte-compiling build/bdist.linux-x86_64/egg/pycocotools/coco.py to coco.cpython-37.pyc\n","byte-compiling build/bdist.linux-x86_64/egg/pycocotools/cocoeval.py to cocoeval.cpython-37.pyc\n","byte-compiling build/bdist.linux-x86_64/egg/pycocotools/mask.py to mask.cpython-37.pyc\n","creating stub loader for pycocotools/_mask.cpython-37m-x86_64-linux-gnu.so\n","byte-compiling build/bdist.linux-x86_64/egg/pycocotools/_mask.py to _mask.cpython-37.pyc\n","creating build/bdist.linux-x86_64/egg/EGG-INFO\n","copying pycocotools.egg-info/PKG-INFO -> build/bdist.linux-x86_64/egg/EGG-INFO\n","copying pycocotools.egg-info/SOURCES.txt -> build/bdist.linux-x86_64/egg/EGG-INFO\n","copying pycocotools.egg-info/dependency_links.txt -> build/bdist.linux-x86_64/egg/EGG-INFO\n","copying pycocotools.egg-info/requires.txt -> build/bdist.linux-x86_64/egg/EGG-INFO\n","copying pycocotools.egg-info/top_level.txt -> build/bdist.linux-x86_64/egg/EGG-INFO\n","writing build/bdist.linux-x86_64/egg/EGG-INFO/native_libs.txt\n","zip_safe flag not set; analyzing archive contents...\n","pycocotools.__pycache__._mask.cpython-37: module references __file__\n","creating 'dist/pycocotools-2.0-py3.7-linux-x86_64.egg' and adding 'build/bdist.linux-x86_64/egg' to it\n","removing 'build/bdist.linux-x86_64/egg' (and everything under it)\n","Processing pycocotools-2.0-py3.7-linux-x86_64.egg\n","creating /usr/local/lib/python3.7/dist-packages/pycocotools-2.0-py3.7-linux-x86_64.egg\n","Extracting pycocotools-2.0-py3.7-linux-x86_64.egg to /usr/local/lib/python3.7/dist-packages\n","Adding pycocotools 2.0 to easy-install.pth file\n","\n","Installed /usr/local/lib/python3.7/dist-packages/pycocotools-2.0-py3.7-linux-x86_64.egg\n","Processing dependencies for pycocotools==2.0\n","Searching for matplotlib==3.2.2\n","Best match: matplotlib 3.2.2\n","Adding matplotlib 3.2.2 to easy-install.pth file\n","\n","Using /usr/local/lib/python3.7/dist-packages\n","Searching for Cython==0.29.24\n","Best match: Cython 0.29.24\n","Adding Cython 0.29.24 to easy-install.pth file\n","Installing cygdb script to /usr/local/bin\n","Installing cython script to /usr/local/bin\n","Installing cythonize script to /usr/local/bin\n","\n","Using /usr/local/lib/python3.7/dist-packages\n","Searching for setuptools==57.4.0\n","Best match: setuptools 57.4.0\n","Adding setuptools 57.4.0 to easy-install.pth file\n","\n","Using /usr/local/lib/python3.7/dist-packages\n","Searching for numpy==1.19.5\n","Best match: numpy 1.19.5\n","Adding numpy 1.19.5 to easy-install.pth file\n","Installing f2py script to /usr/local/bin\n","Installing f2py3 script to /usr/local/bin\n","Installing f2py3.7 script to /usr/local/bin\n","\n","Using /usr/local/lib/python3.7/dist-packages\n","Searching for cycler==0.10.0\n","Best match: cycler 0.10.0\n","Adding cycler 0.10.0 to easy-install.pth file\n","\n","Using /usr/local/lib/python3.7/dist-packages\n","Searching for pyparsing==2.4.7\n","Best match: pyparsing 2.4.7\n","Adding pyparsing 2.4.7 to easy-install.pth file\n","\n","Using /usr/local/lib/python3.7/dist-packages\n","Searching for kiwisolver==1.3.2\n","Best match: kiwisolver 1.3.2\n","Adding kiwisolver 1.3.2 to easy-install.pth file\n","\n","Using /usr/local/lib/python3.7/dist-packages\n","Searching for python-dateutil==2.8.2\n","Best match: python-dateutil 2.8.2\n","Adding python-dateutil 2.8.2 to easy-install.pth file\n","\n","Using /usr/local/lib/python3.7/dist-packages\n","Searching for six==1.15.0\n","Best match: six 1.15.0\n","Adding six 1.15.0 to easy-install.pth file\n","\n","Using /usr/local/lib/python3.7/dist-packages\n","Finished processing dependencies for pycocotools==2.0\n","rm -rf build\n"]}],"source":["os.chdir('cocoapi/PythonAPI/')\n","! make install\n","os.chdir('../../')"]},{"cell_type":"markdown","metadata":{"id":"ZVfFMstwI9YY"},"source":["## Replicate Research Test Results\n","\n","This section of the notebook compiles all the commands used in order to obtain the evaluation results of the HRNet-W32 & HRNet-W48 models on the COCO 2017 Validation Dataset. This section also contains the comparison of the results between the use of the Ground Truth Person Bounding Boxes and the Predicted Person Bounding Boxes on the COCO 2017 Validation Dataset."]},{"cell_type":"markdown","metadata":{"id":"DyJZtTd2da3-"},"source":["### Predicted Person Bounding Boxes"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":11693,"status":"ok","timestamp":1635855359253,"user":{"displayName":"David Miguel Barles Dela Cruz","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"16734584818421110252"},"user_tz":-660},"id":"dQzrJOeTpgOO","outputId":"e2157f82-c2a5-403a-d8aa-c3d51abd07e9"},"outputs":[{"name":"stdout","output_type":"stream","text":["=> creating output/coco/pose_hrnet/w32_384x288_adam_lr1e-3\n","=> creating log/coco/pose_hrnet/w32_384x288_adam_lr1e-3_2021-11-02-12-10\n","Namespace(cfg='experiments/coco/hrnet/w32_384x288_adam_lr1e-3.yaml', dataDir='', logDir='', modelDir='', opts=['TEST.MODEL_FILE', 'models/pytorch/pose_coco/pose_hrnet_w32_384x288.pth', 'TEST.USE_GT_BBOX', 'False'], prevModelDir='')\n","AUTO_RESUME: True\n","CUDNN:\n","  BENCHMARK: True\n","  DETERMINISTIC: False\n","  ENABLED: True\n","DATASET:\n","  COLOR_RGB: True\n","  DATASET: coco\n","  DATA_FORMAT: jpg\n","  FLIP: True\n","  HYBRID_JOINTS_TYPE: \n","  NUM_JOINTS_HALF_BODY: 8\n","  PROB_HALF_BODY: 0.3\n","  ROOT: data/coco/\n","  ROT_FACTOR: 45\n","  SCALE_FACTOR: 0.35\n","  SELECT_DATA: False\n","  TEST_SET: val2017\n","  TRAIN_SET: train2017\n","DATA_DIR: \n","DEBUG:\n","  DEBUG: True\n","  SAVE_BATCH_IMAGES_GT: True\n","  SAVE_BATCH_IMAGES_PRED: True\n","  SAVE_HEATMAPS_GT: True\n","  SAVE_HEATMAPS_PRED: True\n","GPUS: (0, 1, 2, 3)\n","LOG_DIR: log\n","LOSS:\n","  TOPK: 8\n","  USE_DIFFERENT_JOINTS_WEIGHT: False\n","  USE_OHKM: False\n","  USE_TARGET_WEIGHT: True\n","MODEL:\n","  EXTRA:\n","    FINAL_CONV_KERNEL: 1\n","    PRETRAINED_LAYERS: ['conv1', 'bn1', 'conv2', 'bn2', 'layer1', 'transition1', 'stage2', 'transition2', 'stage3', 'transition3', 'stage4']\n","    STAGE2:\n","      BLOCK: BASIC\n","      FUSE_METHOD: SUM\n","      NUM_BLOCKS: [4, 4]\n","      NUM_BRANCHES: 2\n","      NUM_CHANNELS: [32, 64]\n","      NUM_MODULES: 1\n","    STAGE3:\n","      BLOCK: BASIC\n","      FUSE_METHOD: SUM\n","      NUM_BLOCKS: [4, 4, 4]\n","      NUM_BRANCHES: 3\n","      NUM_CHANNELS: [32, 64, 128]\n","      NUM_MODULES: 4\n","    STAGE4:\n","      BLOCK: BASIC\n","      FUSE_METHOD: SUM\n","      NUM_BLOCKS: [4, 4, 4, 4]\n","      NUM_BRANCHES: 4\n","      NUM_CHANNELS: [32, 64, 128, 256]\n","      NUM_MODULES: 3\n","  HEATMAP_SIZE: [72, 96]\n","  IMAGE_SIZE: [288, 384]\n","  INIT_WEIGHTS: True\n","  NAME: pose_hrnet\n","  NUM_JOINTS: 17\n","  PRETRAINED: models/pytorch/imagenet/hrnet_w32-36af842e.pth\n","  SIGMA: 3\n","  TAG_PER_JOINT: True\n","  TARGET_TYPE: gaussian\n","OUTPUT_DIR: output\n","PIN_MEMORY: True\n","PRINT_FREQ: 100\n","RANK: 0\n","TEST:\n","  BATCH_SIZE_PER_GPU: 32\n","  BBOX_THRE: 1.0\n","  COCO_BBOX_FILE: data/coco/person_detection_results/COCO_val2017_detections_AP_H_56_person.json\n","  FLIP_TEST: True\n","  IMAGE_THRE: 0.0\n","  IN_VIS_THRE: 0.2\n","  MODEL_FILE: models/pytorch/pose_coco/pose_hrnet_w32_384x288.pth\n","  NMS_THRE: 1.0\n","  OKS_THRE: 0.9\n","  POST_PROCESS: True\n","  SHIFT_HEATMAP: True\n","  SOFT_NMS: False\n","  USE_GT_BBOX: False\n","TRAIN:\n","  BATCH_SIZE_PER_GPU: 32\n","  BEGIN_EPOCH: 0\n","  CHECKPOINT: \n","  END_EPOCH: 210\n","  GAMMA1: 0.99\n","  GAMMA2: 0.0\n","  LR: 0.001\n","  LR_FACTOR: 0.1\n","  LR_STEP: [170, 200]\n","  MOMENTUM: 0.9\n","  NESTEROV: False\n","  OPTIMIZER: adam\n","  RESUME: False\n","  SHUFFLE: True\n","  WD: 0.0001\n","WORKERS: 2\n","=> loading model from models/pytorch/pose_coco/pose_hrnet_w32_384x288.pth\n","loading annotations into memory...\n","Done (t=0.30s)\n","creating index...\n","index created!\n","=> classes: ['__background__', 'person']\n","=> num_images: 5000\n","=> Total boxes: 104125\n","=> Total boxes after fliter low score@0.0: 104125\n","=> load 104125 samples\n","Test: [0/814]\tTime 19.602 (19.602)\tLoss 0.0002 (0.0002)\tAccuracy 0.000 (0.000)\n","Traceback (most recent call last):\n","  File \"tools/test.py\", line 130, in <module>\n","    main()\n","  File \"tools/test.py\", line 126, in main\n","    final_output_dir, tb_log_dir)\n","  File \"/content/gdrive/My Drive/ColabNotebooks/ADS_Project/deep-high-resolution-net.pytorch-master/tools/../lib/core/function.py\", line 135, in validate\n","    output_flipped = flip_back(output_flipped.cpu().numpy(),\n","KeyboardInterrupt\n"]}],"source":["!python tools/test.py \\\n","    --cfg experiments/coco/hrnet/w32_384x288_adam_lr1e-3.yaml \\\n","    TEST.MODEL_FILE models/pytorch/pose_coco/pose_hrnet_w32_384x288.pth \\\n","    TEST.USE_GT_BBOX False"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":8661353,"status":"ok","timestamp":1635864029279,"user":{"displayName":"David Miguel Barles Dela Cruz","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"16734584818421110252"},"user_tz":-660},"id":"LKGAm4pJEQEU","outputId":"b4fdc4ef-0288-4788-b040-1552f5445f40"},"outputs":[{"name":"stdout","output_type":"stream","text":["=> creating output/coco/pose_hrnet/w48_384x288_adam_lr1e-3\n","=> creating log/coco/pose_hrnet/w48_384x288_adam_lr1e-3_2021-11-02-12-16\n","Namespace(cfg='experiments/coco/hrnet/w48_384x288_adam_lr1e-3.yaml', dataDir='', logDir='', modelDir='', opts=['TEST.MODEL_FILE', 'models/pytorch/pose_coco/pose_hrnet_w48_384x288.pth', 'TEST.USE_GT_BBOX', 'False'], prevModelDir='')\n","AUTO_RESUME: True\n","CUDNN:\n","  BENCHMARK: True\n","  DETERMINISTIC: False\n","  ENABLED: True\n","DATASET:\n","  COLOR_RGB: True\n","  DATASET: coco\n","  DATA_FORMAT: jpg\n","  FLIP: True\n","  HYBRID_JOINTS_TYPE: \n","  NUM_JOINTS_HALF_BODY: 8\n","  PROB_HALF_BODY: 0.3\n","  ROOT: data/coco/\n","  ROT_FACTOR: 45\n","  SCALE_FACTOR: 0.35\n","  SELECT_DATA: False\n","  TEST_SET: val2017\n","  TRAIN_SET: train2017\n","DATA_DIR: \n","DEBUG:\n","  DEBUG: True\n","  SAVE_BATCH_IMAGES_GT: True\n","  SAVE_BATCH_IMAGES_PRED: True\n","  SAVE_HEATMAPS_GT: True\n","  SAVE_HEATMAPS_PRED: True\n","GPUS: (0, 1, 2, 3)\n","LOG_DIR: log\n","LOSS:\n","  TOPK: 8\n","  USE_DIFFERENT_JOINTS_WEIGHT: False\n","  USE_OHKM: False\n","  USE_TARGET_WEIGHT: True\n","MODEL:\n","  EXTRA:\n","    FINAL_CONV_KERNEL: 1\n","    PRETRAINED_LAYERS: ['conv1', 'bn1', 'conv2', 'bn2', 'layer1', 'transition1', 'stage2', 'transition2', 'stage3', 'transition3', 'stage4']\n","    STAGE2:\n","      BLOCK: BASIC\n","      FUSE_METHOD: SUM\n","      NUM_BLOCKS: [4, 4]\n","      NUM_BRANCHES: 2\n","      NUM_CHANNELS: [48, 96]\n","      NUM_MODULES: 1\n","    STAGE3:\n","      BLOCK: BASIC\n","      FUSE_METHOD: SUM\n","      NUM_BLOCKS: [4, 4, 4]\n","      NUM_BRANCHES: 3\n","      NUM_CHANNELS: [48, 96, 192]\n","      NUM_MODULES: 4\n","    STAGE4:\n","      BLOCK: BASIC\n","      FUSE_METHOD: SUM\n","      NUM_BLOCKS: [4, 4, 4, 4]\n","      NUM_BRANCHES: 4\n","      NUM_CHANNELS: [48, 96, 192, 384]\n","      NUM_MODULES: 3\n","  HEATMAP_SIZE: [72, 96]\n","  IMAGE_SIZE: [288, 384]\n","  INIT_WEIGHTS: True\n","  NAME: pose_hrnet\n","  NUM_JOINTS: 17\n","  PRETRAINED: models/pytorch/imagenet/hrnet_w48-8ef0771d.pth\n","  SIGMA: 3\n","  TAG_PER_JOINT: True\n","  TARGET_TYPE: gaussian\n","OUTPUT_DIR: output\n","PIN_MEMORY: True\n","PRINT_FREQ: 100\n","RANK: 0\n","TEST:\n","  BATCH_SIZE_PER_GPU: 24\n","  BBOX_THRE: 1.0\n","  COCO_BBOX_FILE: data/coco/person_detection_results/COCO_val2017_detections_AP_H_56_person.json\n","  FLIP_TEST: True\n","  IMAGE_THRE: 0.0\n","  IN_VIS_THRE: 0.2\n","  MODEL_FILE: models/pytorch/pose_coco/pose_hrnet_w48_384x288.pth\n","  NMS_THRE: 1.0\n","  OKS_THRE: 0.9\n","  POST_PROCESS: True\n","  SHIFT_HEATMAP: True\n","  SOFT_NMS: False\n","  USE_GT_BBOX: False\n","TRAIN:\n","  BATCH_SIZE_PER_GPU: 24\n","  BEGIN_EPOCH: 0\n","  CHECKPOINT: \n","  END_EPOCH: 210\n","  GAMMA1: 0.99\n","  GAMMA2: 0.0\n","  LR: 0.001\n","  LR_FACTOR: 0.1\n","  LR_STEP: [170, 200]\n","  MOMENTUM: 0.9\n","  NESTEROV: False\n","  OPTIMIZER: adam\n","  RESUME: False\n","  SHUFFLE: True\n","  WD: 0.0001\n","WORKERS: 12\n","=> loading model from models/pytorch/pose_coco/pose_hrnet_w48_384x288.pth\n","loading annotations into memory...\n","Done (t=0.31s)\n","creating index...\n","index created!\n","=> classes: ['__background__', 'person']\n","=> num_images: 5000\n","=> Total boxes: 104125\n","=> Total boxes after fliter low score@0.0: 104125\n","=> load 104125 samples\n","/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 12 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","  cpuset_checked))\n","Test: [0/1085]\tTime 39.083 (39.083)\tLoss 0.0002 (0.0002)\tAccuracy 0.000 (0.000)\n","Test: [100/1085]\tTime 7.721 (8.100)\tLoss 0.0004 (0.0001)\tAccuracy 0.000 (0.004)\n","Test: [200/1085]\tTime 7.768 (7.956)\tLoss 0.0000 (0.0001)\tAccuracy 0.000 (0.004)\n","Test: [300/1085]\tTime 7.703 (7.909)\tLoss 0.0001 (0.0001)\tAccuracy 0.000 (0.006)\n","Test: [400/1085]\tTime 7.723 (7.881)\tLoss 0.0000 (0.0001)\tAccuracy 0.000 (0.005)\n","Test: [500/1085]\tTime 7.719 (7.866)\tLoss 0.0006 (0.0001)\tAccuracy 0.000 (0.004)\n","Test: [600/1085]\tTime 7.697 (7.854)\tLoss 0.0001 (0.0001)\tAccuracy 0.000 (0.005)\n","Test: [700/1085]\tTime 7.720 (7.851)\tLoss 0.0000 (0.0001)\tAccuracy 0.000 (0.004)\n","Test: [800/1085]\tTime 7.724 (7.845)\tLoss 0.0000 (0.0001)\tAccuracy 0.000 (0.004)\n","Test: [900/1085]\tTime 7.734 (7.839)\tLoss 0.0000 (0.0001)\tAccuracy 0.000 (0.004)\n","Test: [1000/1085]\tTime 7.704 (7.834)\tLoss 0.0003 (0.0001)\tAccuracy 0.000 (0.003)\n","=> writing results json to output/coco/pose_hrnet/w48_384x288_adam_lr1e-3/results/keypoints_val2017_results_0.json\n","Loading and preparing results...\n","DONE (t=5.53s)\n","creating index...\n","index created!\n","Running per image evaluation...\n","Evaluate annotation type *keypoints*\n","DONE (t=16.71s).\n","Accumulating evaluation results...\n","DONE (t=0.40s).\n"," Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets= 20 ] = 0.763\n"," Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets= 20 ] = 0.908\n"," Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets= 20 ] = 0.829\n"," Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets= 20 ] = 0.723\n"," Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets= 20 ] = 0.834\n"," Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 20 ] = 0.812\n"," Average Recall     (AR) @[ IoU=0.50      | area=   all | maxDets= 20 ] = 0.942\n"," Average Recall     (AR) @[ IoU=0.75      | area=   all | maxDets= 20 ] = 0.871\n"," Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets= 20 ] = 0.767\n"," Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets= 20 ] = 0.876\n","| Arch | AP | Ap .5 | AP .75 | AP (M) | AP (L) | AR | AR .5 | AR .75 | AR (M) | AR (L) |\n","|---|---|---|---|---|---|---|---|---|---|---|\n","| pose_hrnet | 0.763 | 0.908 | 0.829 | 0.723 | 0.834 | 0.812 | 0.942 | 0.871 | 0.767 | 0.876 |\n"]}],"source":[" !python tools/test.py \\\n","    --cfg experiments/coco/hrnet/w48_384x288_adam_lr1e-3.yaml \\\n","    TEST.MODEL_FILE models/pytorch/pose_coco/pose_hrnet_w48_384x288.pth \\\n","    TEST.USE_GT_BBOX False"]},{"cell_type":"markdown","metadata":{"id":"u0nR4-h_deHv"},"source":["### Ground Truth Person Bounding Boxes"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ov_SekxWdgpO"},"outputs":[],"source":["!python tools/test.py \\\n","    --cfg experiments/coco/hrnet/w32_384x288_adam_lr1e-3.yaml \\\n","    TEST.MODEL_FILE models/pytorch/pose_coco/pose_hrnet_w32_384x288.pth \\\n","    TEST.USE_GT_BBOX True"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"lbxQsC62dgdU"},"outputs":[],"source":["!python tools/test.py \\\n","    --cfg experiments/coco/hrnet/w48_384x288_adam_lr1e-3.yaml \\\n","    TEST.MODEL_FILE models/pytorch/pose_coco/pose_hrnet_w48_384x288.pth \\\n","    TEST.USE_GT_BBOX True"]},{"cell_type":"markdown","metadata":{"id":"k-y8o1aWnj_z"},"source":["## New Dataset Test Results\n","\n","This section of the notebook compiles all the commands used in order to obtain the evaluation results of the HRNet-W32 & HRNet-W48 models on the New Dataset. This contains the results on the Full Dataset as well as the Orientation-based Datasets:\n","* Front-Facing Dataset (David)\n","* Back-Facing Dataset (Srikar)\n","* Side-Facing Dataset (Shreyas)\n","* Multiple People Dataset (Rigel)"]},{"cell_type":"markdown","metadata":{"id":"Ig2fW05jsfvD"},"source":["### Full Dataset Images"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"NLoGYH5kssQr"},"outputs":[],"source":["#HRNET W32_384x288\n","!python tools/test.py \\\n","    --cfg experiments/coco/hrnet/ADS_Config/ADS_COCO_ALL_IMAGES_w32_384x288_adam_lr1e-3.yaml \\\n","    TEST.MODEL_FILE models/pytorch/pose_coco/pose_hrnet_w32_384x288.pth \\\n","    TEST.USE_GT_BBOX True"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"BWi5waY9sstw"},"outputs":[],"source":["#HRNET W48_384x288\n","!python tools/test.py \\\n","    --cfg experiments/coco/hrnet/ADS_Config/ADS_COCO_ALL_IMAGES_w48_384x288_adam_lr1e-3.yaml \\\n","    TEST.MODEL_FILE models/pytorch/pose_coco/pose_hrnet_w48_384x288.pth \\\n","    TEST.USE_GT_BBOX True"]},{"cell_type":"markdown","metadata":{"id":"02XLo4dqsiyJ"},"source":["### Front Images"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":20431,"status":"ok","timestamp":1635759708767,"user":{"displayName":"David Miguel Barles Dela Cruz","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"16734584818421110252"},"user_tz":-660},"id":"CIkZSCaAuBp1","outputId":"7747c7f6-0a73-4e86-d760-a175ed70ac5d"},"outputs":[{"name":"stdout","output_type":"stream","text":["=> creating output/coco/pose_hrnet/ADS_COCO_FRONT_IMAGES_w32_384x288_adam_lr1e-3\n","=> creating log/coco/pose_hrnet/ADS_COCO_FRONT_IMAGES_w32_384x288_adam_lr1e-3_2021-11-01-09-41\n","Namespace(cfg='experiments/coco/hrnet/ADS_Config/ADS_COCO_FRONT_IMAGES_w32_384x288_adam_lr1e-3.yaml', dataDir='', logDir='', modelDir='', opts=['TEST.MODEL_FILE', 'models/pytorch/pose_coco/pose_hrnet_w32_384x288.pth', 'TEST.USE_GT_BBOX', 'True'], prevModelDir='')\n","AUTO_RESUME: True\n","CUDNN:\n","  BENCHMARK: True\n","  DETERMINISTIC: False\n","  ENABLED: True\n","DATASET:\n","  COLOR_RGB: True\n","  DATASET: coco\n","  DATA_FORMAT: jpg\n","  FLIP: True\n","  HYBRID_JOINTS_TYPE: \n","  NUM_JOINTS_HALF_BODY: 8\n","  PROB_HALF_BODY: 0.3\n","  ROOT: data/coco/\n","  ROT_FACTOR: 45\n","  SCALE_FACTOR: 0.35\n","  SELECT_DATA: False\n","  TEST_SET: ads_project_front\n","  TRAIN_SET: train2017\n","DATA_DIR: \n","DEBUG:\n","  DEBUG: True\n","  SAVE_BATCH_IMAGES_GT: True\n","  SAVE_BATCH_IMAGES_PRED: True\n","  SAVE_HEATMAPS_GT: True\n","  SAVE_HEATMAPS_PRED: True\n","GPUS: (0, 1, 2, 3)\n","LOG_DIR: log\n","LOSS:\n","  TOPK: 8\n","  USE_DIFFERENT_JOINTS_WEIGHT: False\n","  USE_OHKM: False\n","  USE_TARGET_WEIGHT: True\n","MODEL:\n","  EXTRA:\n","    FINAL_CONV_KERNEL: 1\n","    PRETRAINED_LAYERS: ['conv1', 'bn1', 'conv2', 'bn2', 'layer1', 'transition1', 'stage2', 'transition2', 'stage3', 'transition3', 'stage4']\n","    STAGE2:\n","      BLOCK: BASIC\n","      FUSE_METHOD: SUM\n","      NUM_BLOCKS: [4, 4]\n","      NUM_BRANCHES: 2\n","      NUM_CHANNELS: [32, 64]\n","      NUM_MODULES: 1\n","    STAGE3:\n","      BLOCK: BASIC\n","      FUSE_METHOD: SUM\n","      NUM_BLOCKS: [4, 4, 4]\n","      NUM_BRANCHES: 3\n","      NUM_CHANNELS: [32, 64, 128]\n","      NUM_MODULES: 4\n","    STAGE4:\n","      BLOCK: BASIC\n","      FUSE_METHOD: SUM\n","      NUM_BLOCKS: [4, 4, 4, 4]\n","      NUM_BRANCHES: 4\n","      NUM_CHANNELS: [32, 64, 128, 256]\n","      NUM_MODULES: 3\n","  HEATMAP_SIZE: [72, 96]\n","  IMAGE_SIZE: [288, 384]\n","  INIT_WEIGHTS: True\n","  NAME: pose_hrnet\n","  NUM_JOINTS: 17\n","  PRETRAINED: models/pytorch/imagenet/hrnet_w32-36af842e.pth\n","  SIGMA: 3\n","  TAG_PER_JOINT: True\n","  TARGET_TYPE: gaussian\n","OUTPUT_DIR: output\n","PIN_MEMORY: True\n","PRINT_FREQ: 100\n","RANK: 0\n","TEST:\n","  BATCH_SIZE_PER_GPU: 32\n","  BBOX_THRE: 1.0\n","  COCO_BBOX_FILE: data/coco/person_detection_results/COCO_val2017_detections_AP_H_56_person.json\n","  FLIP_TEST: True\n","  IMAGE_THRE: 0.0\n","  IN_VIS_THRE: 0.2\n","  MODEL_FILE: models/pytorch/pose_coco/pose_hrnet_w32_384x288.pth\n","  NMS_THRE: 1.0\n","  OKS_THRE: 0.9\n","  POST_PROCESS: True\n","  SHIFT_HEATMAP: True\n","  SOFT_NMS: False\n","  USE_GT_BBOX: True\n","TRAIN:\n","  BATCH_SIZE_PER_GPU: 32\n","  BEGIN_EPOCH: 0\n","  CHECKPOINT: \n","  END_EPOCH: 210\n","  GAMMA1: 0.99\n","  GAMMA2: 0.0\n","  LR: 0.001\n","  LR_FACTOR: 0.1\n","  LR_STEP: [170, 200]\n","  MOMENTUM: 0.9\n","  NESTEROV: False\n","  OPTIMIZER: adam\n","  RESUME: False\n","  SHUFFLE: True\n","  WD: 0.0001\n","WORKERS: 2\n","=> loading model from models/pytorch/pose_coco/pose_hrnet_w32_384x288.pth\n","loading annotations into memory...\n","Done (t=0.17s)\n","creating index...\n","index created!\n","=> classes: ['__background__', 'person']\n","=> num_images: 23\n","=> load 33 samples\n","data/coco/images/ads_project_front/000000000104.jpg\n","\n","0\n","data/coco/images/ads_project_front/000000000106.jpg\n","\n","0\n","data/coco/images/ads_project_front/000000000106.jpg\n","\n","0\n","data/coco/images/ads_project_front/000000000107.jpg\n","\n","0\n","data/coco/images/ads_project_front/000000000108.jpg\n","\n","0\n","data/coco/images/ads_project_front/000000000109.jpg\n","\n","0\n","data/coco/images/ads_project_front/000000000110.jpg\n","\n","0\n","data/coco/images/ads_project_front/000000000110.jpg\n","\n","0\n","data/coco/images/ads_project_front/000000000111.jpg\n","\n","0\n","data/coco/images/ads_project_front/000000000111.jpg\n","\n","0\n","data/coco/images/ads_project_front/000000000145.jpg\n","\n","0\n","data/coco/images/ads_project_front/000000000145.jpg\n","\n","0\n","data/coco/images/ads_project_front/000000000146.jpg\n","\n","0\n","data/coco/images/ads_project_front/000000000146.jpg\n","\n","0\n","data/coco/images/ads_project_front/000000000147.jpg\n","\n","0\n","data/coco/images/ads_project_front/000000000148.jpg\n","\n","0\n","data/coco/images/ads_project_front/000000000149.jpg\n","\n","0\n","data/coco/images/ads_project_front/000000000150.jpg\n","\n","0\n","data/coco/images/ads_project_front/000000000150.jpg\n","\n","0\n","data/coco/images/ads_project_front/000000000150.jpg\n","\n","0\n","data/coco/images/ads_project_front/000000000150.jpg\n","\n","0\n","data/coco/images/ads_project_front/000000000150.jpg\n","\n","0\n","data/coco/images/ads_project_front/000000000151.jpg\n","\n","0\n","data/coco/images/ads_project_front/000000000152.jpg\n","\n","0\n","data/coco/images/ads_project_front/000000000153.jpg\n","\n","0\n","data/coco/images/ads_project_front/000000000154.jpg\n","\n","0\n","data/coco/images/ads_project_front/000000000155.jpg\n","\n","0\n","data/coco/images/ads_project_front/000000000155.jpg\n","\n","0\n","data/coco/images/ads_project_front/000000000156.jpg\n","\n","0\n","data/coco/images/ads_project_front/000000000157.jpg\n","\n","0\n","data/coco/images/ads_project_front/000000000158.jpg\n","\n","0\n","data/coco/images/ads_project_front/000000000159.jpg\n","\n","0\n","data/coco/images/ads_project_front/000000000160.jpg\n","\n","0\n","Test: [0/1]\tTime 10.152 (10.152)\tLoss 0.0003 (0.0003)\tAccuracy 0.947 (0.947)\n","=> writing results json to output/coco/pose_hrnet/ADS_COCO_FRONT_IMAGES_w32_384x288_adam_lr1e-3/results/keypoints_ads_project_front_results_0.json\n","Loading and preparing results...\n","DONE (t=0.00s)\n","creating index...\n","index created!\n","Running per image evaluation...\n","Evaluate annotation type *keypoints*\n","DONE (t=0.01s).\n","Accumulating evaluation results...\n","DONE (t=0.00s).\n"," Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets= 20 ] = 0.959\n"," Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets= 20 ] = 1.000\n"," Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets= 20 ] = 1.000\n"," Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets= 20 ] = 0.917\n"," Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets= 20 ] = 0.969\n"," Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 20 ] = 0.967\n"," Average Recall     (AR) @[ IoU=0.50      | area=   all | maxDets= 20 ] = 1.000\n"," Average Recall     (AR) @[ IoU=0.75      | area=   all | maxDets= 20 ] = 1.000\n"," Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets= 20 ] = 0.933\n"," Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets= 20 ] = 0.972\n","| Arch | AP | Ap .5 | AP .75 | AP (M) | AP (L) | AR | AR .5 | AR .75 | AR (M) | AR (L) |\n","|---|---|---|---|---|---|---|---|---|---|---|\n","| pose_hrnet | 0.959 | 1.000 | 1.000 | 0.917 | 0.969 | 0.967 | 1.000 | 1.000 | 0.933 | 0.972 |\n"]}],"source":["#HRNET W32_384x288\n","!python tools/test.py \\\n","    --cfg experiments/coco/hrnet/ADS_Config/ADS_COCO_FRONT_IMAGES_w32_384x288_adam_lr1e-3.yaml \\\n","    TEST.MODEL_FILE models/pytorch/pose_coco/pose_hrnet_w32_384x288.pth \\\n","    TEST.USE_GT_BBOX True"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":17546,"status":"ok","timestamp":1635759726305,"user":{"displayName":"David Miguel Barles Dela Cruz","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"16734584818421110252"},"user_tz":-660},"id":"cYgqU2DiuBgD","outputId":"b713333f-fe9e-4e83-8872-583328e7d6d2"},"outputs":[{"name":"stdout","output_type":"stream","text":["=> creating output/coco/pose_hrnet/ADS_COCO_FRONT_IMAGES_w48_384x288_adam_lr1e-3\n","=> creating log/coco/pose_hrnet/ADS_COCO_FRONT_IMAGES_w48_384x288_adam_lr1e-3_2021-11-01-09-41\n","Namespace(cfg='experiments/coco/hrnet/ADS_Config/ADS_COCO_FRONT_IMAGES_w48_384x288_adam_lr1e-3.yaml', dataDir='', logDir='', modelDir='', opts=['TEST.MODEL_FILE', 'models/pytorch/pose_coco/pose_hrnet_w48_384x288.pth', 'TEST.USE_GT_BBOX', 'True'], prevModelDir='')\n","AUTO_RESUME: True\n","CUDNN:\n","  BENCHMARK: True\n","  DETERMINISTIC: False\n","  ENABLED: True\n","DATASET:\n","  COLOR_RGB: True\n","  DATASET: coco\n","  DATA_FORMAT: jpg\n","  FLIP: True\n","  HYBRID_JOINTS_TYPE: \n","  NUM_JOINTS_HALF_BODY: 8\n","  PROB_HALF_BODY: 0.3\n","  ROOT: data/coco/\n","  ROT_FACTOR: 45\n","  SCALE_FACTOR: 0.35\n","  SELECT_DATA: False\n","  TEST_SET: ads_project_front\n","  TRAIN_SET: train2017\n","DATA_DIR: \n","DEBUG:\n","  DEBUG: True\n","  SAVE_BATCH_IMAGES_GT: True\n","  SAVE_BATCH_IMAGES_PRED: True\n","  SAVE_HEATMAPS_GT: True\n","  SAVE_HEATMAPS_PRED: True\n","GPUS: (0, 1, 2, 3)\n","LOG_DIR: log\n","LOSS:\n","  TOPK: 8\n","  USE_DIFFERENT_JOINTS_WEIGHT: False\n","  USE_OHKM: False\n","  USE_TARGET_WEIGHT: True\n","MODEL:\n","  EXTRA:\n","    FINAL_CONV_KERNEL: 1\n","    PRETRAINED_LAYERS: ['conv1', 'bn1', 'conv2', 'bn2', 'layer1', 'transition1', 'stage2', 'transition2', 'stage3', 'transition3', 'stage4']\n","    STAGE2:\n","      BLOCK: BASIC\n","      FUSE_METHOD: SUM\n","      NUM_BLOCKS: [4, 4]\n","      NUM_BRANCHES: 2\n","      NUM_CHANNELS: [48, 96]\n","      NUM_MODULES: 1\n","    STAGE3:\n","      BLOCK: BASIC\n","      FUSE_METHOD: SUM\n","      NUM_BLOCKS: [4, 4, 4]\n","      NUM_BRANCHES: 3\n","      NUM_CHANNELS: [48, 96, 192]\n","      NUM_MODULES: 4\n","    STAGE4:\n","      BLOCK: BASIC\n","      FUSE_METHOD: SUM\n","      NUM_BLOCKS: [4, 4, 4, 4]\n","      NUM_BRANCHES: 4\n","      NUM_CHANNELS: [48, 96, 192, 384]\n","      NUM_MODULES: 3\n","  HEATMAP_SIZE: [72, 96]\n","  IMAGE_SIZE: [288, 384]\n","  INIT_WEIGHTS: True\n","  NAME: pose_hrnet\n","  NUM_JOINTS: 17\n","  PRETRAINED: models/pytorch/imagenet/hrnet_w48-8ef0771d.pth\n","  SIGMA: 3\n","  TAG_PER_JOINT: True\n","  TARGET_TYPE: gaussian\n","OUTPUT_DIR: output\n","PIN_MEMORY: True\n","PRINT_FREQ: 100\n","RANK: 0\n","TEST:\n","  BATCH_SIZE_PER_GPU: 24\n","  BBOX_THRE: 1.0\n","  COCO_BBOX_FILE: data/coco/person_detection_results/COCO_val2017_detections_AP_H_56_person.json\n","  FLIP_TEST: True\n","  IMAGE_THRE: 0.0\n","  IN_VIS_THRE: 0.2\n","  MODEL_FILE: models/pytorch/pose_coco/pose_hrnet_w48_384x288.pth\n","  NMS_THRE: 1.0\n","  OKS_THRE: 0.9\n","  POST_PROCESS: True\n","  SHIFT_HEATMAP: True\n","  SOFT_NMS: False\n","  USE_GT_BBOX: True\n","TRAIN:\n","  BATCH_SIZE_PER_GPU: 24\n","  BEGIN_EPOCH: 0\n","  CHECKPOINT: \n","  END_EPOCH: 210\n","  GAMMA1: 0.99\n","  GAMMA2: 0.0\n","  LR: 0.001\n","  LR_FACTOR: 0.1\n","  LR_STEP: [170, 200]\n","  MOMENTUM: 0.9\n","  NESTEROV: False\n","  OPTIMIZER: adam\n","  RESUME: False\n","  SHUFFLE: True\n","  WD: 0.0001\n","WORKERS: 12\n","=> loading model from models/pytorch/pose_coco/pose_hrnet_w48_384x288.pth\n","loading annotations into memory...\n","Done (t=0.00s)\n","creating index...\n","index created!\n","=> classes: ['__background__', 'person']\n","=> num_images: 23\n","=> load 33 samples\n","/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 12 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","  cpuset_checked))\n","data/coco/images/ads_project_front/000000000104.jpg\n","\n","0\n","data/coco/images/ads_project_front/000000000106.jpg\n","\n","0\n","data/coco/images/ads_project_front/000000000106.jpg\n","\n","0\n","data/coco/images/ads_project_front/000000000107.jpg\n","\n","0\n","data/coco/images/ads_project_front/000000000108.jpg\n","\n","0\n","data/coco/images/ads_project_front/000000000109.jpg\n","\n","0\n","data/coco/images/ads_project_front/000000000110.jpg\n","\n","0\n","data/coco/images/ads_project_front/000000000110.jpg\n","\n","0\n","data/coco/images/ads_project_front/000000000111.jpg\n","\n","0\n","data/coco/images/ads_project_front/000000000111.jpg\n","\n","0\n","data/coco/images/ads_project_front/000000000145.jpg\n","\n","0\n","data/coco/images/ads_project_front/000000000145.jpg\n","\n","0\n","data/coco/images/ads_project_front/000000000146.jpg\n","\n","0\n","data/coco/images/ads_project_front/000000000146.jpg\n","\n","0\n","data/coco/images/ads_project_front/000000000147.jpg\n","\n","0\n","data/coco/images/ads_project_front/000000000148.jpg\n","\n","0\n","data/coco/images/ads_project_front/000000000149.jpg\n","\n","0\n","data/coco/images/ads_project_front/000000000150.jpg\n","\n","0\n","data/coco/images/ads_project_front/000000000150.jpg\n","\n","0\n","data/coco/images/ads_project_front/000000000150.jpg\n","\n","0\n","data/coco/images/ads_project_front/000000000150.jpg\n","\n","0\n","data/coco/images/ads_project_front/000000000150.jpg\n","\n","0\n","data/coco/images/ads_project_front/000000000151.jpg\n","\n","0\n","data/coco/images/ads_project_front/000000000152.jpg\n","\n","0\n","data/coco/images/ads_project_front/000000000153.jpg\n","\n","0\n","data/coco/images/ads_project_front/000000000154.jpg\n","\n","0\n","data/coco/images/ads_project_front/000000000155.jpg\n","\n","0\n","data/coco/images/ads_project_front/000000000155.jpg\n","\n","0\n","data/coco/images/ads_project_front/000000000156.jpg\n","\n","0\n","data/coco/images/ads_project_front/000000000157.jpg\n","\n","0\n","data/coco/images/ads_project_front/000000000158.jpg\n","\n","0\n","data/coco/images/ads_project_front/000000000159.jpg\n","\n","0\n","data/coco/images/ads_project_front/000000000160.jpg\n","\n","0\n","Test: [0/1]\tTime 7.741 (7.741)\tLoss 0.0002 (0.0002)\tAccuracy 0.951 (0.951)\n","=> writing results json to output/coco/pose_hrnet/ADS_COCO_FRONT_IMAGES_w48_384x288_adam_lr1e-3/results/keypoints_ads_project_front_results_0.json\n","Loading and preparing results...\n","DONE (t=0.00s)\n","creating index...\n","index created!\n","Running per image evaluation...\n","Evaluate annotation type *keypoints*\n","DONE (t=0.01s).\n","Accumulating evaluation results...\n","DONE (t=0.00s).\n"," Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets= 20 ] = 0.967\n"," Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets= 20 ] = 1.000\n"," Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets= 20 ] = 1.000\n"," Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets= 20 ] = 0.944\n"," Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets= 20 ] = 0.974\n"," Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 20 ] = 0.973\n"," Average Recall     (AR) @[ IoU=0.50      | area=   all | maxDets= 20 ] = 1.000\n"," Average Recall     (AR) @[ IoU=0.75      | area=   all | maxDets= 20 ] = 1.000\n"," Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets= 20 ] = 0.967\n"," Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets= 20 ] = 0.976\n","| Arch | AP | Ap .5 | AP .75 | AP (M) | AP (L) | AR | AR .5 | AR .75 | AR (M) | AR (L) |\n","|---|---|---|---|---|---|---|---|---|---|---|\n","| pose_hrnet | 0.967 | 1.000 | 1.000 | 0.944 | 0.974 | 0.973 | 1.000 | 1.000 | 0.967 | 0.976 |\n"]}],"source":["#HRNET W48_384x288\n","!python tools/test.py \\\n","    --cfg experiments/coco/hrnet/ADS_Config/ADS_COCO_FRONT_IMAGES_w48_384x288_adam_lr1e-3.yaml \\\n","    TEST.MODEL_FILE models/pytorch/pose_coco/pose_hrnet_w48_384x288.pth \\\n","    TEST.USE_GT_BBOX True"]},{"cell_type":"markdown","metadata":{"id":"IZTSXcT0wsGf"},"source":["### Back Images"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":17978,"status":"ok","timestamp":1635758546661,"user":{"displayName":"Sai Mahesh Srikar Parimi","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"13573223867659676765"},"user_tz":-660},"id":"zGuunkAtxpVv","outputId":"1a7f6b0c-bc7e-4277-d7b3-180fa8afb5f1"},"outputs":[{"name":"stdout","output_type":"stream","text":["=> creating output/coco/pose_hrnet/ADS_COCO_BACK_IMAGES_w32_384x288_adam_lr1e-3\n","=> creating log/coco/pose_hrnet/ADS_COCO_BACK_IMAGES_w32_384x288_adam_lr1e-3_2021-11-01-09-22\n","Namespace(cfg='experiments/coco/hrnet/ADS_Config/ADS_COCO_BACK_IMAGES_w32_384x288_adam_lr1e-3.yaml', dataDir='', logDir='', modelDir='', opts=['TEST.MODEL_FILE', 'models/pytorch/pose_coco/pose_hrnet_w32_384x288.pth', 'TEST.USE_GT_BBOX', 'True'], prevModelDir='')\n","AUTO_RESUME: True\n","CUDNN:\n","  BENCHMARK: True\n","  DETERMINISTIC: False\n","  ENABLED: True\n","DATASET:\n","  COLOR_RGB: True\n","  DATASET: coco\n","  DATA_FORMAT: jpg\n","  FLIP: True\n","  HYBRID_JOINTS_TYPE: \n","  NUM_JOINTS_HALF_BODY: 8\n","  PROB_HALF_BODY: 0.3\n","  ROOT: data/coco/\n","  ROT_FACTOR: 45\n","  SCALE_FACTOR: 0.35\n","  SELECT_DATA: False\n","  TEST_SET: ads_project_back\n","  TRAIN_SET: train2017\n","DATA_DIR: \n","DEBUG:\n","  DEBUG: True\n","  SAVE_BATCH_IMAGES_GT: True\n","  SAVE_BATCH_IMAGES_PRED: True\n","  SAVE_HEATMAPS_GT: True\n","  SAVE_HEATMAPS_PRED: True\n","GPUS: (0, 1, 2, 3)\n","LOG_DIR: log\n","LOSS:\n","  TOPK: 8\n","  USE_DIFFERENT_JOINTS_WEIGHT: False\n","  USE_OHKM: False\n","  USE_TARGET_WEIGHT: True\n","MODEL:\n","  EXTRA:\n","    FINAL_CONV_KERNEL: 1\n","    PRETRAINED_LAYERS: ['conv1', 'bn1', 'conv2', 'bn2', 'layer1', 'transition1', 'stage2', 'transition2', 'stage3', 'transition3', 'stage4']\n","    STAGE2:\n","      BLOCK: BASIC\n","      FUSE_METHOD: SUM\n","      NUM_BLOCKS: [4, 4]\n","      NUM_BRANCHES: 2\n","      NUM_CHANNELS: [32, 64]\n","      NUM_MODULES: 1\n","    STAGE3:\n","      BLOCK: BASIC\n","      FUSE_METHOD: SUM\n","      NUM_BLOCKS: [4, 4, 4]\n","      NUM_BRANCHES: 3\n","      NUM_CHANNELS: [32, 64, 128]\n","      NUM_MODULES: 4\n","    STAGE4:\n","      BLOCK: BASIC\n","      FUSE_METHOD: SUM\n","      NUM_BLOCKS: [4, 4, 4, 4]\n","      NUM_BRANCHES: 4\n","      NUM_CHANNELS: [32, 64, 128, 256]\n","      NUM_MODULES: 3\n","  HEATMAP_SIZE: [72, 96]\n","  IMAGE_SIZE: [288, 384]\n","  INIT_WEIGHTS: True\n","  NAME: pose_hrnet\n","  NUM_JOINTS: 17\n","  PRETRAINED: models/pytorch/imagenet/hrnet_w32-36af842e.pth\n","  SIGMA: 3\n","  TAG_PER_JOINT: True\n","  TARGET_TYPE: gaussian\n","OUTPUT_DIR: output\n","PIN_MEMORY: True\n","PRINT_FREQ: 100\n","RANK: 0\n","TEST:\n","  BATCH_SIZE_PER_GPU: 32\n","  BBOX_THRE: 1.0\n","  COCO_BBOX_FILE: data/coco/person_detection_results/COCO_val2017_detections_AP_H_56_person.json\n","  FLIP_TEST: True\n","  IMAGE_THRE: 0.0\n","  IN_VIS_THRE: 0.2\n","  MODEL_FILE: models/pytorch/pose_coco/pose_hrnet_w32_384x288.pth\n","  NMS_THRE: 1.0\n","  OKS_THRE: 0.9\n","  POST_PROCESS: True\n","  SHIFT_HEATMAP: True\n","  SOFT_NMS: False\n","  USE_GT_BBOX: True\n","TRAIN:\n","  BATCH_SIZE_PER_GPU: 32\n","  BEGIN_EPOCH: 0\n","  CHECKPOINT: \n","  END_EPOCH: 210\n","  GAMMA1: 0.99\n","  GAMMA2: 0.0\n","  LR: 0.001\n","  LR_FACTOR: 0.1\n","  LR_STEP: [170, 200]\n","  MOMENTUM: 0.9\n","  NESTEROV: False\n","  OPTIMIZER: adam\n","  RESUME: False\n","  SHUFFLE: True\n","  WD: 0.0001\n","WORKERS: 2\n","=> loading model from models/pytorch/pose_coco/pose_hrnet_w32_384x288.pth\n","loading annotations into memory...\n","Done (t=0.32s)\n","creating index...\n","index created!\n","=> classes: ['__background__', 'person']\n","=> num_images: 15\n","=> load 16 samples\n","data/coco/images/ads_project_back/000000000012.jpg\n","\n","0\n","data/coco/images/ads_project_back/000000000013.jpg\n","\n","0\n","data/coco/images/ads_project_back/000000000014.jpg\n","\n","0\n","data/coco/images/ads_project_back/000000000015.jpg\n","\n","0\n","data/coco/images/ads_project_back/000000000016.jpg\n","\n","0\n","data/coco/images/ads_project_back/000000000017.jpg\n","\n","0\n","data/coco/images/ads_project_back/000000000022.jpg\n","\n","0\n","data/coco/images/ads_project_back/000000000023.jpg\n","\n","0\n","data/coco/images/ads_project_back/000000000024.jpg\n","\n","0\n","data/coco/images/ads_project_back/000000000025.jpg\n","\n","0\n","data/coco/images/ads_project_back/000000000099.jpg\n","\n","0\n","data/coco/images/ads_project_back/000000000100.jpg\n","\n","0\n","data/coco/images/ads_project_back/000000000102.jpg\n","\n","0\n","data/coco/images/ads_project_back/000000000103.jpg\n","\n","0\n","data/coco/images/ads_project_back/000000000103.jpg\n","\n","0\n","data/coco/images/ads_project_back/000000000105.jpg\n","\n","0\n","Test: [0/1]\tTime 7.107 (7.107)\tLoss 0.0002 (0.0002)\tAccuracy 0.972 (0.972)\n","=> writing results json to output/coco/pose_hrnet/ADS_COCO_BACK_IMAGES_w32_384x288_adam_lr1e-3/results/keypoints_ads_project_back_results_0.json\n","Loading and preparing results...\n","DONE (t=0.00s)\n","creating index...\n","index created!\n","Running per image evaluation...\n","Evaluate annotation type *keypoints*\n","DONE (t=0.01s).\n","Accumulating evaluation results...\n","DONE (t=0.00s).\n"," Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets= 20 ] = 0.989\n"," Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets= 20 ] = 1.000\n"," Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets= 20 ] = 1.000\n"," Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets= 20 ] = -1.000\n"," Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets= 20 ] = 0.989\n"," Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 20 ] = 0.994\n"," Average Recall     (AR) @[ IoU=0.50      | area=   all | maxDets= 20 ] = 1.000\n"," Average Recall     (AR) @[ IoU=0.75      | area=   all | maxDets= 20 ] = 1.000\n"," Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets= 20 ] = -1.000\n"," Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets= 20 ] = 0.994\n","| Arch | AP | Ap .5 | AP .75 | AP (M) | AP (L) | AR | AR .5 | AR .75 | AR (M) | AR (L) |\n","|---|---|---|---|---|---|---|---|---|---|---|\n","| pose_hrnet | 0.989 | 1.000 | 1.000 | -1.000 | 0.989 | 0.994 | 1.000 | 1.000 | -1.000 | 0.994 |\n"]}],"source":["#HRNET W32_384x288\n","!python tools/test.py \\\n","    --cfg experiments/coco/hrnet/ADS_Config/ADS_COCO_BACK_IMAGES_w32_384x288_adam_lr1e-3.yaml \\\n","    TEST.MODEL_FILE models/pytorch/pose_coco/pose_hrnet_w32_384x288.pth \\\n","    TEST.USE_GT_BBOX True"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":15882,"status":"ok","timestamp":1635758562532,"user":{"displayName":"Sai Mahesh Srikar Parimi","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"13573223867659676765"},"user_tz":-660},"id":"IwDojK6HxpV8","outputId":"c14bf2f5-152e-4d48-8595-918f5758a5e3"},"outputs":[{"name":"stdout","output_type":"stream","text":["=> creating output/coco/pose_hrnet/ADS_COCO_BACK_IMAGES_w48_384x288_adam_lr1e-3\n","=> creating log/coco/pose_hrnet/ADS_COCO_BACK_IMAGES_w48_384x288_adam_lr1e-3_2021-11-01-09-22\n","Namespace(cfg='experiments/coco/hrnet/ADS_Config/ADS_COCO_BACK_IMAGES_w48_384x288_adam_lr1e-3.yaml', dataDir='', logDir='', modelDir='', opts=['TEST.MODEL_FILE', 'models/pytorch/pose_coco/pose_hrnet_w48_384x288.pth', 'TEST.USE_GT_BBOX', 'True'], prevModelDir='')\n","AUTO_RESUME: True\n","CUDNN:\n","  BENCHMARK: True\n","  DETERMINISTIC: False\n","  ENABLED: True\n","DATASET:\n","  COLOR_RGB: True\n","  DATASET: coco\n","  DATA_FORMAT: jpg\n","  FLIP: True\n","  HYBRID_JOINTS_TYPE: \n","  NUM_JOINTS_HALF_BODY: 8\n","  PROB_HALF_BODY: 0.3\n","  ROOT: data/coco/\n","  ROT_FACTOR: 45\n","  SCALE_FACTOR: 0.35\n","  SELECT_DATA: False\n","  TEST_SET: ads_project_back\n","  TRAIN_SET: train2017\n","DATA_DIR: \n","DEBUG:\n","  DEBUG: True\n","  SAVE_BATCH_IMAGES_GT: True\n","  SAVE_BATCH_IMAGES_PRED: True\n","  SAVE_HEATMAPS_GT: True\n","  SAVE_HEATMAPS_PRED: True\n","GPUS: (0, 1, 2, 3)\n","LOG_DIR: log\n","LOSS:\n","  TOPK: 8\n","  USE_DIFFERENT_JOINTS_WEIGHT: False\n","  USE_OHKM: False\n","  USE_TARGET_WEIGHT: True\n","MODEL:\n","  EXTRA:\n","    FINAL_CONV_KERNEL: 1\n","    PRETRAINED_LAYERS: ['conv1', 'bn1', 'conv2', 'bn2', 'layer1', 'transition1', 'stage2', 'transition2', 'stage3', 'transition3', 'stage4']\n","    STAGE2:\n","      BLOCK: BASIC\n","      FUSE_METHOD: SUM\n","      NUM_BLOCKS: [4, 4]\n","      NUM_BRANCHES: 2\n","      NUM_CHANNELS: [48, 96]\n","      NUM_MODULES: 1\n","    STAGE3:\n","      BLOCK: BASIC\n","      FUSE_METHOD: SUM\n","      NUM_BLOCKS: [4, 4, 4]\n","      NUM_BRANCHES: 3\n","      NUM_CHANNELS: [48, 96, 192]\n","      NUM_MODULES: 4\n","    STAGE4:\n","      BLOCK: BASIC\n","      FUSE_METHOD: SUM\n","      NUM_BLOCKS: [4, 4, 4, 4]\n","      NUM_BRANCHES: 4\n","      NUM_CHANNELS: [48, 96, 192, 384]\n","      NUM_MODULES: 3\n","  HEATMAP_SIZE: [72, 96]\n","  IMAGE_SIZE: [288, 384]\n","  INIT_WEIGHTS: True\n","  NAME: pose_hrnet\n","  NUM_JOINTS: 17\n","  PRETRAINED: models/pytorch/imagenet/hrnet_w48-8ef0771d.pth\n","  SIGMA: 3\n","  TAG_PER_JOINT: True\n","  TARGET_TYPE: gaussian\n","OUTPUT_DIR: output\n","PIN_MEMORY: True\n","PRINT_FREQ: 100\n","RANK: 0\n","TEST:\n","  BATCH_SIZE_PER_GPU: 24\n","  BBOX_THRE: 1.0\n","  COCO_BBOX_FILE: data/coco/person_detection_results/COCO_val2017_detections_AP_H_56_person.json\n","  FLIP_TEST: True\n","  IMAGE_THRE: 0.0\n","  IN_VIS_THRE: 0.2\n","  MODEL_FILE: models/pytorch/pose_coco/pose_hrnet_w48_384x288.pth\n","  NMS_THRE: 1.0\n","  OKS_THRE: 0.9\n","  POST_PROCESS: True\n","  SHIFT_HEATMAP: True\n","  SOFT_NMS: False\n","  USE_GT_BBOX: True\n","TRAIN:\n","  BATCH_SIZE_PER_GPU: 24\n","  BEGIN_EPOCH: 0\n","  CHECKPOINT: \n","  END_EPOCH: 210\n","  GAMMA1: 0.99\n","  GAMMA2: 0.0\n","  LR: 0.001\n","  LR_FACTOR: 0.1\n","  LR_STEP: [170, 200]\n","  MOMENTUM: 0.9\n","  NESTEROV: False\n","  OPTIMIZER: adam\n","  RESUME: False\n","  SHUFFLE: True\n","  WD: 0.0001\n","WORKERS: 12\n","=> loading model from models/pytorch/pose_coco/pose_hrnet_w48_384x288.pth\n","loading annotations into memory...\n","Done (t=0.00s)\n","creating index...\n","index created!\n","=> classes: ['__background__', 'person']\n","=> num_images: 15\n","=> load 16 samples\n","/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 12 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","  cpuset_checked))\n","data/coco/images/ads_project_back/000000000012.jpg\n","\n","0\n","data/coco/images/ads_project_back/000000000013.jpg\n","\n","0\n","data/coco/images/ads_project_back/000000000014.jpg\n","\n","0\n","data/coco/images/ads_project_back/000000000015.jpg\n","\n","0\n","data/coco/images/ads_project_back/000000000016.jpg\n","\n","0\n","data/coco/images/ads_project_back/000000000017.jpg\n","\n","0\n","data/coco/images/ads_project_back/000000000022.jpg\n","\n","0\n","data/coco/images/ads_project_back/000000000023.jpg\n","\n","0\n","data/coco/images/ads_project_back/000000000024.jpg\n","\n","0\n","data/coco/images/ads_project_back/000000000025.jpg\n","\n","0\n","data/coco/images/ads_project_back/000000000099.jpg\n","\n","0\n","data/coco/images/ads_project_back/000000000100.jpg\n","\n","0\n","data/coco/images/ads_project_back/000000000102.jpg\n","\n","0\n","data/coco/images/ads_project_back/000000000103.jpg\n","\n","0\n","data/coco/images/ads_project_back/000000000103.jpg\n","\n","0\n","data/coco/images/ads_project_back/000000000105.jpg\n","\n","0\n","Test: [0/1]\tTime 4.083 (4.083)\tLoss 0.0002 (0.0002)\tAccuracy 0.969 (0.969)\n","=> writing results json to output/coco/pose_hrnet/ADS_COCO_BACK_IMAGES_w48_384x288_adam_lr1e-3/results/keypoints_ads_project_back_results_0.json\n","Loading and preparing results...\n","DONE (t=0.00s)\n","creating index...\n","index created!\n","Running per image evaluation...\n","Evaluate annotation type *keypoints*\n","DONE (t=0.01s).\n","Accumulating evaluation results...\n","DONE (t=0.00s).\n"," Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets= 20 ] = 0.990\n"," Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets= 20 ] = 1.000\n"," Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets= 20 ] = 1.000\n"," Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets= 20 ] = -1.000\n"," Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets= 20 ] = 0.990\n"," Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 20 ] = 0.994\n"," Average Recall     (AR) @[ IoU=0.50      | area=   all | maxDets= 20 ] = 1.000\n"," Average Recall     (AR) @[ IoU=0.75      | area=   all | maxDets= 20 ] = 1.000\n"," Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets= 20 ] = -1.000\n"," Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets= 20 ] = 0.994\n","| Arch | AP | Ap .5 | AP .75 | AP (M) | AP (L) | AR | AR .5 | AR .75 | AR (M) | AR (L) |\n","|---|---|---|---|---|---|---|---|---|---|---|\n","| pose_hrnet | 0.990 | 1.000 | 1.000 | -1.000 | 0.990 | 0.994 | 1.000 | 1.000 | -1.000 | 0.994 |\n"]}],"source":["#HRNET W48_384x288\n","!python tools/test.py \\\n","    --cfg experiments/coco/hrnet/ADS_Config/ADS_COCO_BACK_IMAGES_w48_384x288_adam_lr1e-3.yaml \\\n","    TEST.MODEL_FILE models/pytorch/pose_coco/pose_hrnet_w48_384x288.pth \\\n","    TEST.USE_GT_BBOX True"]},{"cell_type":"markdown","metadata":{"id":"_5vdGHELwt65"},"source":["### Side Images"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":15308,"status":"ok","timestamp":1635757395628,"user":{"displayName":"Shreyas Kumar Singh","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"03934754512134323541"},"user_tz":-660},"id":"pOQSNbwNxp3c","outputId":"d6863e13-ed7f-42af-ae27-9dfa019118b0"},"outputs":[{"name":"stdout","output_type":"stream","text":["=> creating output/coco/pose_hrnet/ADS_COCO_SIDE_IMAGES_w32_384x288_adam_lr1e-3\n","=> creating log/coco/pose_hrnet/ADS_COCO_SIDE_IMAGES_w32_384x288_adam_lr1e-3_2021-11-01-09-03\n","Namespace(cfg='experiments/coco/hrnet/ADS_Config/ADS_COCO_SIDE_IMAGES_w32_384x288_adam_lr1e-3.yaml', dataDir='', logDir='', modelDir='', opts=['TEST.MODEL_FILE', 'models/pytorch/pose_coco/pose_hrnet_w32_384x288.pth', 'TEST.USE_GT_BBOX', 'True'], prevModelDir='')\n","AUTO_RESUME: True\n","CUDNN:\n","  BENCHMARK: True\n","  DETERMINISTIC: False\n","  ENABLED: True\n","DATASET:\n","  COLOR_RGB: True\n","  DATASET: coco\n","  DATA_FORMAT: jpg\n","  FLIP: True\n","  HYBRID_JOINTS_TYPE: \n","  NUM_JOINTS_HALF_BODY: 8\n","  PROB_HALF_BODY: 0.3\n","  ROOT: data/coco/\n","  ROT_FACTOR: 45\n","  SCALE_FACTOR: 0.35\n","  SELECT_DATA: False\n","  TEST_SET: ads_project_side\n","  TRAIN_SET: train2017\n","DATA_DIR: \n","DEBUG:\n","  DEBUG: True\n","  SAVE_BATCH_IMAGES_GT: True\n","  SAVE_BATCH_IMAGES_PRED: True\n","  SAVE_HEATMAPS_GT: True\n","  SAVE_HEATMAPS_PRED: True\n","GPUS: (0, 1, 2, 3)\n","LOG_DIR: log\n","LOSS:\n","  TOPK: 8\n","  USE_DIFFERENT_JOINTS_WEIGHT: False\n","  USE_OHKM: False\n","  USE_TARGET_WEIGHT: True\n","MODEL:\n","  EXTRA:\n","    FINAL_CONV_KERNEL: 1\n","    PRETRAINED_LAYERS: ['conv1', 'bn1', 'conv2', 'bn2', 'layer1', 'transition1', 'stage2', 'transition2', 'stage3', 'transition3', 'stage4']\n","    STAGE2:\n","      BLOCK: BASIC\n","      FUSE_METHOD: SUM\n","      NUM_BLOCKS: [4, 4]\n","      NUM_BRANCHES: 2\n","      NUM_CHANNELS: [32, 64]\n","      NUM_MODULES: 1\n","    STAGE3:\n","      BLOCK: BASIC\n","      FUSE_METHOD: SUM\n","      NUM_BLOCKS: [4, 4, 4]\n","      NUM_BRANCHES: 3\n","      NUM_CHANNELS: [32, 64, 128]\n","      NUM_MODULES: 4\n","    STAGE4:\n","      BLOCK: BASIC\n","      FUSE_METHOD: SUM\n","      NUM_BLOCKS: [4, 4, 4, 4]\n","      NUM_BRANCHES: 4\n","      NUM_CHANNELS: [32, 64, 128, 256]\n","      NUM_MODULES: 3\n","  HEATMAP_SIZE: [72, 96]\n","  IMAGE_SIZE: [288, 384]\n","  INIT_WEIGHTS: True\n","  NAME: pose_hrnet\n","  NUM_JOINTS: 17\n","  PRETRAINED: models/pytorch/imagenet/hrnet_w32-36af842e.pth\n","  SIGMA: 3\n","  TAG_PER_JOINT: True\n","  TARGET_TYPE: gaussian\n","OUTPUT_DIR: output\n","PIN_MEMORY: True\n","PRINT_FREQ: 100\n","RANK: 0\n","TEST:\n","  BATCH_SIZE_PER_GPU: 32\n","  BBOX_THRE: 1.0\n","  COCO_BBOX_FILE: data/coco/person_detection_results/COCO_val2017_detections_AP_H_56_person.json\n","  FLIP_TEST: True\n","  IMAGE_THRE: 0.0\n","  IN_VIS_THRE: 0.2\n","  MODEL_FILE: models/pytorch/pose_coco/pose_hrnet_w32_384x288.pth\n","  NMS_THRE: 1.0\n","  OKS_THRE: 0.9\n","  POST_PROCESS: True\n","  SHIFT_HEATMAP: True\n","  SOFT_NMS: False\n","  USE_GT_BBOX: True\n","TRAIN:\n","  BATCH_SIZE_PER_GPU: 32\n","  BEGIN_EPOCH: 0\n","  CHECKPOINT: \n","  END_EPOCH: 210\n","  GAMMA1: 0.99\n","  GAMMA2: 0.0\n","  LR: 0.001\n","  LR_FACTOR: 0.1\n","  LR_STEP: [170, 200]\n","  MOMENTUM: 0.9\n","  NESTEROV: False\n","  OPTIMIZER: adam\n","  RESUME: False\n","  SHUFFLE: True\n","  WD: 0.0001\n","WORKERS: 2\n","=> loading model from models/pytorch/pose_coco/pose_hrnet_w32_384x288.pth\n","loading annotations into memory...\n","Done (t=0.35s)\n","creating index...\n","index created!\n","=> classes: ['__background__', 'person']\n","=> num_images: 15\n","=> load 19 samples\n","data/coco/images/ads_project_side/000000000112.jpg\n","\n","0\n","data/coco/images/ads_project_side/000000000113.jpg\n","\n","0\n","data/coco/images/ads_project_side/000000000114.jpg\n","\n","0\n","data/coco/images/ads_project_side/000000000115.jpg\n","\n","0\n","data/coco/images/ads_project_side/000000000116.jpg\n","\n","0\n","data/coco/images/ads_project_side/000000000117.jpg\n","\n","0\n","data/coco/images/ads_project_side/000000000117.jpg\n","\n","0\n","data/coco/images/ads_project_side/000000000117.jpg\n","\n","0\n","data/coco/images/ads_project_side/000000000118.jpg\n","\n","0\n","data/coco/images/ads_project_side/000000000119.jpg\n","\n","0\n","data/coco/images/ads_project_side/000000000119.jpg\n","\n","0\n","data/coco/images/ads_project_side/000000000120.jpg\n","\n","0\n","data/coco/images/ads_project_side/000000000121.jpg\n","\n","0\n","data/coco/images/ads_project_side/000000000121.jpg\n","\n","0\n","data/coco/images/ads_project_side/000000000122.jpg\n","\n","0\n","data/coco/images/ads_project_side/000000000123.jpg\n","\n","0\n","data/coco/images/ads_project_side/000000000125.jpg\n","\n","0\n","data/coco/images/ads_project_side/000000000126.jpg\n","\n","0\n","data/coco/images/ads_project_side/000000000137.jpg\n","\n","0\n","Test: [0/1]\tTime 7.278 (7.278)\tLoss 0.0002 (0.0002)\tAccuracy 0.959 (0.959)\n","=> writing results json to output/coco/pose_hrnet/ADS_COCO_SIDE_IMAGES_w32_384x288_adam_lr1e-3/results/keypoints_ads_project_side_results_0.json\n","Loading and preparing results...\n","DONE (t=0.00s)\n","creating index...\n","index created!\n","Running per image evaluation...\n","Evaluate annotation type *keypoints*\n","DONE (t=0.01s).\n","Accumulating evaluation results...\n","DONE (t=0.00s).\n"," Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets= 20 ] = 1.000\n"," Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets= 20 ] = 1.000\n"," Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets= 20 ] = 1.000\n"," Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets= 20 ] = -1.000\n"," Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets= 20 ] = 1.000\n"," Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 20 ] = 1.000\n"," Average Recall     (AR) @[ IoU=0.50      | area=   all | maxDets= 20 ] = 1.000\n"," Average Recall     (AR) @[ IoU=0.75      | area=   all | maxDets= 20 ] = 1.000\n"," Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets= 20 ] = -1.000\n"," Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets= 20 ] = 1.000\n","| Arch | AP | Ap .5 | AP .75 | AP (M) | AP (L) | AR | AR .5 | AR .75 | AR (M) | AR (L) |\n","|---|---|---|---|---|---|---|---|---|---|---|\n","| pose_hrnet | 1.000 | 1.000 | 1.000 | -1.000 | 1.000 | 1.000 | 1.000 | 1.000 | -1.000 | 1.000 |\n"]}],"source":["#HRNET W32_384x288\n","!python tools/test.py \\\n","    --cfg experiments/coco/hrnet/ADS_Config/ADS_COCO_SIDE_IMAGES_w32_384x288_adam_lr1e-3.yaml \\\n","    TEST.MODEL_FILE models/pytorch/pose_coco/pose_hrnet_w32_384x288.pth \\\n","    TEST.USE_GT_BBOX True"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":13572,"status":"ok","timestamp":1635757409180,"user":{"displayName":"Shreyas Kumar Singh","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"03934754512134323541"},"user_tz":-660},"id":"aEslinyFxp3d","outputId":"e9136d32-dcf0-430b-f509-f90cc4bd7444"},"outputs":[{"name":"stdout","output_type":"stream","text":["=> creating output/coco/pose_hrnet/ADS_COCO_SIDE_IMAGES_w48_384x288_adam_lr1e-3\n","=> creating log/coco/pose_hrnet/ADS_COCO_SIDE_IMAGES_w48_384x288_adam_lr1e-3_2021-11-01-09-03\n","Namespace(cfg='experiments/coco/hrnet/ADS_Config/ADS_COCO_SIDE_IMAGES_w48_384x288_adam_lr1e-3.yaml', dataDir='', logDir='', modelDir='', opts=['TEST.MODEL_FILE', 'models/pytorch/pose_coco/pose_hrnet_w48_384x288.pth', 'TEST.USE_GT_BBOX', 'True'], prevModelDir='')\n","AUTO_RESUME: True\n","CUDNN:\n","  BENCHMARK: True\n","  DETERMINISTIC: False\n","  ENABLED: True\n","DATASET:\n","  COLOR_RGB: True\n","  DATASET: coco\n","  DATA_FORMAT: jpg\n","  FLIP: True\n","  HYBRID_JOINTS_TYPE: \n","  NUM_JOINTS_HALF_BODY: 8\n","  PROB_HALF_BODY: 0.3\n","  ROOT: data/coco/\n","  ROT_FACTOR: 45\n","  SCALE_FACTOR: 0.35\n","  SELECT_DATA: False\n","  TEST_SET: ads_project_side\n","  TRAIN_SET: train2017\n","DATA_DIR: \n","DEBUG:\n","  DEBUG: True\n","  SAVE_BATCH_IMAGES_GT: True\n","  SAVE_BATCH_IMAGES_PRED: True\n","  SAVE_HEATMAPS_GT: True\n","  SAVE_HEATMAPS_PRED: True\n","GPUS: (0, 1, 2, 3)\n","LOG_DIR: log\n","LOSS:\n","  TOPK: 8\n","  USE_DIFFERENT_JOINTS_WEIGHT: False\n","  USE_OHKM: False\n","  USE_TARGET_WEIGHT: True\n","MODEL:\n","  EXTRA:\n","    FINAL_CONV_KERNEL: 1\n","    PRETRAINED_LAYERS: ['conv1', 'bn1', 'conv2', 'bn2', 'layer1', 'transition1', 'stage2', 'transition2', 'stage3', 'transition3', 'stage4']\n","    STAGE2:\n","      BLOCK: BASIC\n","      FUSE_METHOD: SUM\n","      NUM_BLOCKS: [4, 4]\n","      NUM_BRANCHES: 2\n","      NUM_CHANNELS: [48, 96]\n","      NUM_MODULES: 1\n","    STAGE3:\n","      BLOCK: BASIC\n","      FUSE_METHOD: SUM\n","      NUM_BLOCKS: [4, 4, 4]\n","      NUM_BRANCHES: 3\n","      NUM_CHANNELS: [48, 96, 192]\n","      NUM_MODULES: 4\n","    STAGE4:\n","      BLOCK: BASIC\n","      FUSE_METHOD: SUM\n","      NUM_BLOCKS: [4, 4, 4, 4]\n","      NUM_BRANCHES: 4\n","      NUM_CHANNELS: [48, 96, 192, 384]\n","      NUM_MODULES: 3\n","  HEATMAP_SIZE: [72, 96]\n","  IMAGE_SIZE: [288, 384]\n","  INIT_WEIGHTS: True\n","  NAME: pose_hrnet\n","  NUM_JOINTS: 17\n","  PRETRAINED: models/pytorch/imagenet/hrnet_w48-8ef0771d.pth\n","  SIGMA: 3\n","  TAG_PER_JOINT: True\n","  TARGET_TYPE: gaussian\n","OUTPUT_DIR: output\n","PIN_MEMORY: True\n","PRINT_FREQ: 100\n","RANK: 0\n","TEST:\n","  BATCH_SIZE_PER_GPU: 24\n","  BBOX_THRE: 1.0\n","  COCO_BBOX_FILE: data/coco/person_detection_results/COCO_val2017_detections_AP_H_56_person.json\n","  FLIP_TEST: True\n","  IMAGE_THRE: 0.0\n","  IN_VIS_THRE: 0.2\n","  MODEL_FILE: models/pytorch/pose_coco/pose_hrnet_w48_384x288.pth\n","  NMS_THRE: 1.0\n","  OKS_THRE: 0.9\n","  POST_PROCESS: True\n","  SHIFT_HEATMAP: True\n","  SOFT_NMS: False\n","  USE_GT_BBOX: True\n","TRAIN:\n","  BATCH_SIZE_PER_GPU: 24\n","  BEGIN_EPOCH: 0\n","  CHECKPOINT: \n","  END_EPOCH: 210\n","  GAMMA1: 0.99\n","  GAMMA2: 0.0\n","  LR: 0.001\n","  LR_FACTOR: 0.1\n","  LR_STEP: [170, 200]\n","  MOMENTUM: 0.9\n","  NESTEROV: False\n","  OPTIMIZER: adam\n","  RESUME: False\n","  SHUFFLE: True\n","  WD: 0.0001\n","WORKERS: 12\n","=> loading model from models/pytorch/pose_coco/pose_hrnet_w48_384x288.pth\n","loading annotations into memory...\n","Done (t=0.00s)\n","creating index...\n","index created!\n","=> classes: ['__background__', 'person']\n","=> num_images: 15\n","=> load 19 samples\n","/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 12 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","  cpuset_checked))\n","data/coco/images/ads_project_side/000000000112.jpg\n","\n","0\n","data/coco/images/ads_project_side/000000000113.jpg\n","\n","0\n","data/coco/images/ads_project_side/000000000114.jpg\n","\n","0\n","data/coco/images/ads_project_side/000000000115.jpg\n","\n","0\n","data/coco/images/ads_project_side/000000000116.jpg\n","\n","0\n","data/coco/images/ads_project_side/000000000117.jpg\n","\n","0\n","data/coco/images/ads_project_side/000000000117.jpg\n","\n","0\n","data/coco/images/ads_project_side/000000000117.jpg\n","\n","0\n","data/coco/images/ads_project_side/000000000118.jpg\n","\n","0\n","data/coco/images/ads_project_side/000000000119.jpg\n","\n","0\n","data/coco/images/ads_project_side/000000000119.jpg\n","\n","0\n","data/coco/images/ads_project_side/000000000120.jpg\n","\n","0\n","data/coco/images/ads_project_side/000000000121.jpg\n","\n","0\n","data/coco/images/ads_project_side/000000000121.jpg\n","\n","0\n","data/coco/images/ads_project_side/000000000122.jpg\n","\n","0\n","data/coco/images/ads_project_side/000000000123.jpg\n","\n","0\n","data/coco/images/ads_project_side/000000000125.jpg\n","\n","0\n","data/coco/images/ads_project_side/000000000126.jpg\n","\n","0\n","data/coco/images/ads_project_side/000000000137.jpg\n","\n","0\n","Test: [0/1]\tTime 4.935 (4.935)\tLoss 0.0002 (0.0002)\tAccuracy 0.974 (0.974)\n","=> writing results json to output/coco/pose_hrnet/ADS_COCO_SIDE_IMAGES_w48_384x288_adam_lr1e-3/results/keypoints_ads_project_side_results_0.json\n","Loading and preparing results...\n","DONE (t=0.00s)\n","creating index...\n","index created!\n","Running per image evaluation...\n","Evaluate annotation type *keypoints*\n","DONE (t=0.01s).\n","Accumulating evaluation results...\n","DONE (t=0.00s).\n"," Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets= 20 ] = 0.994\n"," Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets= 20 ] = 1.000\n"," Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets= 20 ] = 1.000\n"," Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets= 20 ] = -1.000\n"," Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets= 20 ] = 0.994\n"," Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 20 ] = 0.995\n"," Average Recall     (AR) @[ IoU=0.50      | area=   all | maxDets= 20 ] = 1.000\n"," Average Recall     (AR) @[ IoU=0.75      | area=   all | maxDets= 20 ] = 1.000\n"," Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets= 20 ] = -1.000\n"," Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets= 20 ] = 0.995\n","| Arch | AP | Ap .5 | AP .75 | AP (M) | AP (L) | AR | AR .5 | AR .75 | AR (M) | AR (L) |\n","|---|---|---|---|---|---|---|---|---|---|---|\n","| pose_hrnet | 0.994 | 1.000 | 1.000 | -1.000 | 0.994 | 0.995 | 1.000 | 1.000 | -1.000 | 0.995 |\n"]}],"source":["#HRNET W48_384x288\n","!python tools/test.py \\\n","    --cfg experiments/coco/hrnet/ADS_Config/ADS_COCO_SIDE_IMAGES_w48_384x288_adam_lr1e-3.yaml \\\n","    TEST.MODEL_FILE models/pytorch/pose_coco/pose_hrnet_w48_384x288.pth \\\n","    TEST.USE_GT_BBOX True"]},{"cell_type":"markdown","metadata":{"id":"wHfc2kIY2ME-"},"source":["### Multiple People"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":67781,"status":"ok","timestamp":1635759456502,"user":{"displayName":"David Miguel Barles Dela Cruz","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"16734584818421110252"},"user_tz":-660},"id":"iUC75K0b2PH1","outputId":"6922bd97-485e-4c16-9907-bc713745aeb8"},"outputs":[{"name":"stdout","output_type":"stream","text":["=> creating output/coco/pose_hrnet/ADS_COCO_MULTIPLE_PEOPLE_IMAGES_w32_384x288_adam_lr1e-3\n","=> creating log/coco/pose_hrnet/ADS_COCO_MULTIPLE_PEOPLE_IMAGES_w32_384x288_adam_lr1e-3_2021-11-01-09-37\n","Namespace(cfg='experiments/coco/hrnet/ADS_Config/ADS_COCO_MULTIPLE_PEOPLE_IMAGES_w32_384x288_adam_lr1e-3.yaml', dataDir='', logDir='', modelDir='', opts=['TEST.MODEL_FILE', 'models/pytorch/pose_coco/pose_hrnet_w32_384x288.pth', 'TEST.USE_GT_BBOX', 'True'], prevModelDir='')\n","AUTO_RESUME: True\n","CUDNN:\n","  BENCHMARK: True\n","  DETERMINISTIC: False\n","  ENABLED: True\n","DATASET:\n","  COLOR_RGB: True\n","  DATASET: coco\n","  DATA_FORMAT: jpg\n","  FLIP: True\n","  HYBRID_JOINTS_TYPE: \n","  NUM_JOINTS_HALF_BODY: 8\n","  PROB_HALF_BODY: 0.3\n","  ROOT: data/coco/\n","  ROT_FACTOR: 45\n","  SCALE_FACTOR: 0.35\n","  SELECT_DATA: False\n","  TEST_SET: ads_project_multiple_people\n","  TRAIN_SET: train2017\n","DATA_DIR: \n","DEBUG:\n","  DEBUG: True\n","  SAVE_BATCH_IMAGES_GT: True\n","  SAVE_BATCH_IMAGES_PRED: True\n","  SAVE_HEATMAPS_GT: True\n","  SAVE_HEATMAPS_PRED: True\n","GPUS: (0, 1, 2, 3)\n","LOG_DIR: log\n","LOSS:\n","  TOPK: 8\n","  USE_DIFFERENT_JOINTS_WEIGHT: False\n","  USE_OHKM: False\n","  USE_TARGET_WEIGHT: True\n","MODEL:\n","  EXTRA:\n","    FINAL_CONV_KERNEL: 1\n","    PRETRAINED_LAYERS: ['conv1', 'bn1', 'conv2', 'bn2', 'layer1', 'transition1', 'stage2', 'transition2', 'stage3', 'transition3', 'stage4']\n","    STAGE2:\n","      BLOCK: BASIC\n","      FUSE_METHOD: SUM\n","      NUM_BLOCKS: [4, 4]\n","      NUM_BRANCHES: 2\n","      NUM_CHANNELS: [32, 64]\n","      NUM_MODULES: 1\n","    STAGE3:\n","      BLOCK: BASIC\n","      FUSE_METHOD: SUM\n","      NUM_BLOCKS: [4, 4, 4]\n","      NUM_BRANCHES: 3\n","      NUM_CHANNELS: [32, 64, 128]\n","      NUM_MODULES: 4\n","    STAGE4:\n","      BLOCK: BASIC\n","      FUSE_METHOD: SUM\n","      NUM_BLOCKS: [4, 4, 4, 4]\n","      NUM_BRANCHES: 4\n","      NUM_CHANNELS: [32, 64, 128, 256]\n","      NUM_MODULES: 3\n","  HEATMAP_SIZE: [72, 96]\n","  IMAGE_SIZE: [288, 384]\n","  INIT_WEIGHTS: True\n","  NAME: pose_hrnet\n","  NUM_JOINTS: 17\n","  PRETRAINED: models/pytorch/imagenet/hrnet_w32-36af842e.pth\n","  SIGMA: 3\n","  TAG_PER_JOINT: True\n","  TARGET_TYPE: gaussian\n","OUTPUT_DIR: output\n","PIN_MEMORY: True\n","PRINT_FREQ: 100\n","RANK: 0\n","TEST:\n","  BATCH_SIZE_PER_GPU: 32\n","  BBOX_THRE: 1.0\n","  COCO_BBOX_FILE: data/coco/person_detection_results/COCO_val2017_detections_AP_H_56_person.json\n","  FLIP_TEST: True\n","  IMAGE_THRE: 0.0\n","  IN_VIS_THRE: 0.2\n","  MODEL_FILE: models/pytorch/pose_coco/pose_hrnet_w32_384x288.pth\n","  NMS_THRE: 1.0\n","  OKS_THRE: 0.9\n","  POST_PROCESS: True\n","  SHIFT_HEATMAP: True\n","  SOFT_NMS: False\n","  USE_GT_BBOX: True\n","TRAIN:\n","  BATCH_SIZE_PER_GPU: 32\n","  BEGIN_EPOCH: 0\n","  CHECKPOINT: \n","  END_EPOCH: 210\n","  GAMMA1: 0.99\n","  GAMMA2: 0.0\n","  LR: 0.001\n","  LR_FACTOR: 0.1\n","  LR_STEP: [170, 200]\n","  MOMENTUM: 0.9\n","  NESTEROV: False\n","  OPTIMIZER: adam\n","  RESUME: False\n","  SHUFFLE: True\n","  WD: 0.0001\n","WORKERS: 2\n","=> loading model from models/pytorch/pose_coco/pose_hrnet_w32_384x288.pth\n","loading annotations into memory...\n","Done (t=0.71s)\n","creating index...\n","index created!\n","=> classes: ['__background__', 'person']\n","=> num_images: 15\n","=> load 31 samples\n","data/coco/images/ads_project_multiple_people/000000000161.jpg\n","\n","0\n","data/coco/images/ads_project_multiple_people/000000000161.jpg\n","\n","0\n","data/coco/images/ads_project_multiple_people/000000000162.jpg\n","\n","0\n","data/coco/images/ads_project_multiple_people/000000000162.jpg\n","\n","0\n","data/coco/images/ads_project_multiple_people/000000000163.jpg\n","\n","0\n","data/coco/images/ads_project_multiple_people/000000000163.jpg\n","\n","0\n","data/coco/images/ads_project_multiple_people/000000000163.jpg\n","\n","0\n","data/coco/images/ads_project_multiple_people/000000000164.jpg\n","\n","0\n","data/coco/images/ads_project_multiple_people/000000000164.jpg\n","\n","0\n","data/coco/images/ads_project_multiple_people/000000000165.jpg\n","\n","0\n","data/coco/images/ads_project_multiple_people/000000000165.jpg\n","\n","0\n","data/coco/images/ads_project_multiple_people/000000000166.jpg\n","\n","0\n","data/coco/images/ads_project_multiple_people/000000000166.jpg\n","\n","0\n","data/coco/images/ads_project_multiple_people/000000000167.jpg\n","\n","0\n","data/coco/images/ads_project_multiple_people/000000000167.jpg\n","\n","0\n","data/coco/images/ads_project_multiple_people/000000000168.jpg\n","\n","0\n","data/coco/images/ads_project_multiple_people/000000000168.jpg\n","\n","0\n","data/coco/images/ads_project_multiple_people/000000000169.jpg\n","\n","0\n","data/coco/images/ads_project_multiple_people/000000000169.jpg\n","\n","0\n","data/coco/images/ads_project_multiple_people/000000000170.jpg\n","\n","0\n","data/coco/images/ads_project_multiple_people/000000000170.jpg\n","\n","0\n","data/coco/images/ads_project_multiple_people/000000000171.jpg\n","\n","0\n","data/coco/images/ads_project_multiple_people/000000000171.jpg\n","\n","0\n","data/coco/images/ads_project_multiple_people/000000000172.jpg\n","\n","0\n","data/coco/images/ads_project_multiple_people/000000000172.jpg\n","\n","0\n","data/coco/images/ads_project_multiple_people/000000000173.jpg\n","\n","0\n","data/coco/images/ads_project_multiple_people/000000000173.jpg\n","\n","0\n","data/coco/images/ads_project_multiple_people/000000000174.jpg\n","\n","0\n","data/coco/images/ads_project_multiple_people/000000000174.jpg\n","\n","0\n","data/coco/images/ads_project_multiple_people/000000000175.jpg\n","\n","0\n","data/coco/images/ads_project_multiple_people/000000000175.jpg\n","\n","0\n","Test: [0/1]\tTime 12.126 (12.126)\tLoss 0.0003 (0.0003)\tAccuracy 0.894 (0.894)\n","=> writing results json to output/coco/pose_hrnet/ADS_COCO_MULTIPLE_PEOPLE_IMAGES_w32_384x288_adam_lr1e-3/results/keypoints_ads_project_multiple_people_results_0.json\n","Loading and preparing results...\n","DONE (t=0.00s)\n","creating index...\n","index created!\n","Running per image evaluation...\n","Evaluate annotation type *keypoints*\n","DONE (t=0.01s).\n","Accumulating evaluation results...\n","DONE (t=0.00s).\n"," Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets= 20 ] = 0.952\n"," Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets= 20 ] = 1.000\n"," Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets= 20 ] = 1.000\n"," Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets= 20 ] = 1.000\n"," Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets= 20 ] = 0.952\n"," Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 20 ] = 0.965\n"," Average Recall     (AR) @[ IoU=0.50      | area=   all | maxDets= 20 ] = 1.000\n"," Average Recall     (AR) @[ IoU=0.75      | area=   all | maxDets= 20 ] = 1.000\n"," Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets= 20 ] = 1.000\n"," Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets= 20 ] = 0.963\n","| Arch | AP | Ap .5 | AP .75 | AP (M) | AP (L) | AR | AR .5 | AR .75 | AR (M) | AR (L) |\n","|---|---|---|---|---|---|---|---|---|---|---|\n","| pose_hrnet | 0.952 | 1.000 | 1.000 | 1.000 | 0.952 | 0.965 | 1.000 | 1.000 | 1.000 | 0.963 |\n"]}],"source":["#HRNET W32_384x288\n","!python tools/test.py \\\n","    --cfg experiments/coco/hrnet/ADS_Config/ADS_COCO_MULTIPLE_PEOPLE_IMAGES_w32_384x288_adam_lr1e-3.yaml \\\n","    TEST.MODEL_FILE models/pytorch/pose_coco/pose_hrnet_w32_384x288.pth \\\n","    TEST.USE_GT_BBOX True"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":20223,"status":"ok","timestamp":1635759537611,"user":{"displayName":"David Miguel Barles Dela Cruz","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"16734584818421110252"},"user_tz":-660},"id":"qWukde9T2PH1","outputId":"16dd4212-8504-4e53-93b7-07b793573ae5"},"outputs":[{"name":"stdout","output_type":"stream","text":["=> creating output/coco/pose_hrnet/ADS_COCO_MULTIPLE_PEOPLE_IMAGES_w48_384x288_adam_lr1e-3\n","=> creating log/coco/pose_hrnet/ADS_COCO_MULTIPLE_PEOPLE_IMAGES_w48_384x288_adam_lr1e-3_2021-11-01-09-38\n","Namespace(cfg='experiments/coco/hrnet/ADS_Config/ADS_COCO_MULTIPLE_PEOPLE_IMAGES_w48_384x288_adam_lr1e-3.yaml', dataDir='', logDir='', modelDir='', opts=['TEST.MODEL_FILE', 'models/pytorch/pose_coco/pose_hrnet_w48_384x288.pth', 'TEST.USE_GT_BBOX', 'True'], prevModelDir='')\n","AUTO_RESUME: True\n","CUDNN:\n","  BENCHMARK: True\n","  DETERMINISTIC: False\n","  ENABLED: True\n","DATASET:\n","  COLOR_RGB: True\n","  DATASET: coco\n","  DATA_FORMAT: jpg\n","  FLIP: True\n","  HYBRID_JOINTS_TYPE: \n","  NUM_JOINTS_HALF_BODY: 8\n","  PROB_HALF_BODY: 0.3\n","  ROOT: data/coco/\n","  ROT_FACTOR: 45\n","  SCALE_FACTOR: 0.35\n","  SELECT_DATA: False\n","  TEST_SET: ads_project_multiple_people\n","  TRAIN_SET: train2017\n","DATA_DIR: \n","DEBUG:\n","  DEBUG: True\n","  SAVE_BATCH_IMAGES_GT: True\n","  SAVE_BATCH_IMAGES_PRED: True\n","  SAVE_HEATMAPS_GT: True\n","  SAVE_HEATMAPS_PRED: True\n","GPUS: (0, 1, 2, 3)\n","LOG_DIR: log\n","LOSS:\n","  TOPK: 8\n","  USE_DIFFERENT_JOINTS_WEIGHT: False\n","  USE_OHKM: False\n","  USE_TARGET_WEIGHT: True\n","MODEL:\n","  EXTRA:\n","    FINAL_CONV_KERNEL: 1\n","    PRETRAINED_LAYERS: ['conv1', 'bn1', 'conv2', 'bn2', 'layer1', 'transition1', 'stage2', 'transition2', 'stage3', 'transition3', 'stage4']\n","    STAGE2:\n","      BLOCK: BASIC\n","      FUSE_METHOD: SUM\n","      NUM_BLOCKS: [4, 4]\n","      NUM_BRANCHES: 2\n","      NUM_CHANNELS: [48, 96]\n","      NUM_MODULES: 1\n","    STAGE3:\n","      BLOCK: BASIC\n","      FUSE_METHOD: SUM\n","      NUM_BLOCKS: [4, 4, 4]\n","      NUM_BRANCHES: 3\n","      NUM_CHANNELS: [48, 96, 192]\n","      NUM_MODULES: 4\n","    STAGE4:\n","      BLOCK: BASIC\n","      FUSE_METHOD: SUM\n","      NUM_BLOCKS: [4, 4, 4, 4]\n","      NUM_BRANCHES: 4\n","      NUM_CHANNELS: [48, 96, 192, 384]\n","      NUM_MODULES: 3\n","  HEATMAP_SIZE: [72, 96]\n","  IMAGE_SIZE: [288, 384]\n","  INIT_WEIGHTS: True\n","  NAME: pose_hrnet\n","  NUM_JOINTS: 17\n","  PRETRAINED: models/pytorch/imagenet/hrnet_w48-8ef0771d.pth\n","  SIGMA: 3\n","  TAG_PER_JOINT: True\n","  TARGET_TYPE: gaussian\n","OUTPUT_DIR: output\n","PIN_MEMORY: True\n","PRINT_FREQ: 100\n","RANK: 0\n","TEST:\n","  BATCH_SIZE_PER_GPU: 24\n","  BBOX_THRE: 1.0\n","  COCO_BBOX_FILE: data/coco/person_detection_results/COCO_val2017_detections_AP_H_56_person.json\n","  FLIP_TEST: True\n","  IMAGE_THRE: 0.0\n","  IN_VIS_THRE: 0.2\n","  MODEL_FILE: models/pytorch/pose_coco/pose_hrnet_w48_384x288.pth\n","  NMS_THRE: 1.0\n","  OKS_THRE: 0.9\n","  POST_PROCESS: True\n","  SHIFT_HEATMAP: True\n","  SOFT_NMS: False\n","  USE_GT_BBOX: True\n","TRAIN:\n","  BATCH_SIZE_PER_GPU: 24\n","  BEGIN_EPOCH: 0\n","  CHECKPOINT: \n","  END_EPOCH: 210\n","  GAMMA1: 0.99\n","  GAMMA2: 0.0\n","  LR: 0.001\n","  LR_FACTOR: 0.1\n","  LR_STEP: [170, 200]\n","  MOMENTUM: 0.9\n","  NESTEROV: False\n","  OPTIMIZER: adam\n","  RESUME: False\n","  SHUFFLE: True\n","  WD: 0.0001\n","WORKERS: 12\n","=> loading model from models/pytorch/pose_coco/pose_hrnet_w48_384x288.pth\n","loading annotations into memory...\n","Done (t=0.00s)\n","creating index...\n","index created!\n","=> classes: ['__background__', 'person']\n","=> num_images: 15\n","=> load 31 samples\n","/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 12 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","  cpuset_checked))\n","data/coco/images/ads_project_multiple_people/000000000161.jpg\n","\n","0\n","data/coco/images/ads_project_multiple_people/000000000161.jpg\n","\n","0\n","data/coco/images/ads_project_multiple_people/000000000162.jpg\n","\n","0\n","data/coco/images/ads_project_multiple_people/000000000162.jpg\n","\n","0\n","data/coco/images/ads_project_multiple_people/000000000163.jpg\n","\n","0\n","data/coco/images/ads_project_multiple_people/000000000163.jpg\n","\n","0\n","data/coco/images/ads_project_multiple_people/000000000163.jpg\n","\n","0\n","data/coco/images/ads_project_multiple_people/000000000164.jpg\n","\n","0\n","data/coco/images/ads_project_multiple_people/000000000164.jpg\n","\n","0\n","data/coco/images/ads_project_multiple_people/000000000165.jpg\n","\n","0\n","data/coco/images/ads_project_multiple_people/000000000165.jpg\n","\n","0\n","data/coco/images/ads_project_multiple_people/000000000166.jpg\n","\n","0\n","data/coco/images/ads_project_multiple_people/000000000166.jpg\n","\n","0\n","data/coco/images/ads_project_multiple_people/000000000167.jpg\n","\n","0\n","data/coco/images/ads_project_multiple_people/000000000167.jpg\n","\n","0\n","data/coco/images/ads_project_multiple_people/000000000168.jpg\n","\n","0\n","data/coco/images/ads_project_multiple_people/000000000168.jpg\n","\n","0\n","data/coco/images/ads_project_multiple_people/000000000169.jpg\n","\n","0\n","data/coco/images/ads_project_multiple_people/000000000169.jpg\n","\n","0\n","data/coco/images/ads_project_multiple_people/000000000170.jpg\n","\n","0\n","data/coco/images/ads_project_multiple_people/000000000170.jpg\n","\n","0\n","data/coco/images/ads_project_multiple_people/000000000171.jpg\n","\n","0\n","data/coco/images/ads_project_multiple_people/000000000171.jpg\n","\n","0\n","data/coco/images/ads_project_multiple_people/000000000172.jpg\n","\n","0\n","data/coco/images/ads_project_multiple_people/000000000172.jpg\n","\n","0\n","data/coco/images/ads_project_multiple_people/000000000173.jpg\n","\n","0\n","data/coco/images/ads_project_multiple_people/000000000173.jpg\n","\n","0\n","data/coco/images/ads_project_multiple_people/000000000174.jpg\n","\n","0\n","data/coco/images/ads_project_multiple_people/000000000174.jpg\n","\n","0\n","data/coco/images/ads_project_multiple_people/000000000175.jpg\n","\n","0\n","data/coco/images/ads_project_multiple_people/000000000175.jpg\n","\n","0\n","Test: [0/1]\tTime 7.954 (7.954)\tLoss 0.0003 (0.0003)\tAccuracy 0.896 (0.896)\n","=> writing results json to output/coco/pose_hrnet/ADS_COCO_MULTIPLE_PEOPLE_IMAGES_w48_384x288_adam_lr1e-3/results/keypoints_ads_project_multiple_people_results_0.json\n","Loading and preparing results...\n","DONE (t=0.00s)\n","creating index...\n","index created!\n","Running per image evaluation...\n","Evaluate annotation type *keypoints*\n","DONE (t=0.01s).\n","Accumulating evaluation results...\n","DONE (t=0.00s).\n"," Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets= 20 ] = 0.948\n"," Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets= 20 ] = 1.000\n"," Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets= 20 ] = 1.000\n"," Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets= 20 ] = 1.000\n"," Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets= 20 ] = 0.948\n"," Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 20 ] = 0.961\n"," Average Recall     (AR) @[ IoU=0.50      | area=   all | maxDets= 20 ] = 1.000\n"," Average Recall     (AR) @[ IoU=0.75      | area=   all | maxDets= 20 ] = 1.000\n"," Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets= 20 ] = 1.000\n"," Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets= 20 ] = 0.960\n","| Arch | AP | Ap .5 | AP .75 | AP (M) | AP (L) | AR | AR .5 | AR .75 | AR (M) | AR (L) |\n","|---|---|---|---|---|---|---|---|---|---|---|\n","| pose_hrnet | 0.948 | 1.000 | 1.000 | 1.000 | 0.948 | 0.961 | 1.000 | 1.000 | 1.000 | 0.960 |\n"]}],"source":["#HRNET W48_384x288\n","!python tools/test.py \\\n","    --cfg experiments/coco/hrnet/ADS_Config/ADS_COCO_MULTIPLE_PEOPLE_IMAGES_w48_384x288_adam_lr1e-3.yaml \\\n","    TEST.MODEL_FILE models/pytorch/pose_coco/pose_hrnet_w48_384x288.pth \\\n","    TEST.USE_GT_BBOX True"]},{"cell_type":"markdown","metadata":{"id":"HKoXyPBn6uUq"},"source":["## Visualize Results\n","\n","This section of the notebook compiles the Linux commands to be used in order to obtain the visualization images using the predicted keypoint annotations JSON File. The main visualization script is the plot_coco.py script which requires the ff. inputs:\n","* Ground Truth Annotation JSON File\n","* Predict Keypoint Annotations JSON File\n","* Output Folder for Visualization Images\n","\n","From there, the script will be responsible in creating the Visualization images using the Predicted Keypoint Annotations. It is important to note that the visualization script considers the OKS scores for each person and will only annotate the Keypoints that are within a certain OKS threshold."]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":108382,"status":"ok","timestamp":1635590657311,"user":{"displayName":"David Miguel Barles Dela Cruz","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"16734584818421110252"},"user_tz":-660},"id":"igUzF4tKHqXx","outputId":"ca648f20-6e6c-4ec0-9a9d-951bd4bcb059"},"outputs":[{"name":"stdout","output_type":"stream","text":["loading annotations into memory...\n","Done (t=0.01s)\n","creating index...\n","index created!\n","Loading and preparing results...\n","DONE (t=0.01s)\n","creating index...\n","index created!\n","000000000001\n","iou:  0.6652609775670467\n","000000000002\n","iou:  0.6763796665147291\n","iou:  0.3702299064188109\n","iou:  0.3065134991830722\n","iou:  0.7522101430383203\n","000000000003\n","iou:  0.37156323143988795\n","iou:  0.681031130439053\n","000000000004\n","iou:  0.41937036540925926\n","000000000005\n","iou:  0.3590074948718093\n","iou:  0.5236898056652826\n","000000000006\n","iou:  0.5962662140870617\n","iou:  0.3132856680440895\n","iou:  0.1685233817153328\n","iou:  0.2165400080625761\n","iou:  0.47635530741659493\n","iou:  0.541600911345005\n","iou:  0.3194022981117718\n","iou:  0.6129050826141513\n","iou:  0.3808772702032126\n","iou:  0.574394888495846\n","iou:  0.19541364154022506\n","000000000007\n","iou:  0.5186791741320473\n","000000000008\n","iou:  0.7587238757263584\n","iou:  0.17788255979363557\n","iou:  0.20105217444620713\n","iou:  0.6205791924451752\n","000000000009\n","iou:  0.547133518297298\n","000000000010\n","iou:  0.2267814566692348\n","iou:  0.6538970979854476\n","iou:  0.6441865530687709\n","iou:  0.37403127081880866\n","000000000011\n","iou:  0.2550468777727685\n","iou:  0.6516231832900392\n","iou:  0.16514620026763066\n","iou:  0.3678153469214736\n","iou:  0.1410617561399495\n","iou:  0.6132399736239976\n","iou:  0.4783237769004228\n","iou:  0.49845591568551384\n","iou:  0.20855765387393876\n","iou:  0.4630892318468233\n","iou:  0.22636646044691539\n","iou:  0.17929687246200102\n","iou:  0.4003778503524458\n","iou:  0.5259342575975022\n","000000000018\n","iou:  0.46219732651482925\n","iou:  0.4069715730710908\n","iou:  0.12172264491104409\n","iou:  0.5902087309614167\n","000000000019\n","iou:  0.6239298675289636\n","000000000020\n","iou:  0.43070377130882703\n","000000000021\n","iou:  0.5365954232485729\n","000000000026\n","iou:  0.510389532420452\n","000000000027\n","iou:  0.3688952540005328\n","000000000028\n","iou:  0.48814724774539553\n","000000000029\n","iou:  0.7218117613407942\n","000000000030\n","iou:  0.6435963317127296\n","000000000031\n","iou:  0.5697242919164672\n","000000000032\n","iou:  0.5206106155523432\n","000000000033\n","iou:  0.7012817023355975\n","000000000034\n","iou:  0.332589926361178\n","iou:  0.18134957542869423\n","iou:  0.8225168438960578\n","iou:  0.6206667657115577\n","iou:  0.4942888143160014\n","iou:  0.44561545880824643\n","iou:  0.5076268775704452\n","iou:  0.7709169179621523\n","iou:  0.1884495486230363\n","000000000035\n","iou:  0.49192909861032313\n","iou:  0.3471786498271539\n","iou:  0.32396345385474823\n","iou:  0.5752486350513399\n","iou:  0.5822237224507503\n","000000000036\n","iou:  0.5409055497276715\n","000000000037\n","iou:  0.5158878047553589\n","000000000038\n","iou:  0.5773017664414936\n","000000000039\n","iou:  0.4221644941615494\n","000000000040\n","iou:  0.34320241389813333\n","iou:  0.4527321809804996\n","iou:  0.5358461705189655\n","iou:  0.2752500421552618\n","000000000041\n","iou:  0.295887366632953\n","iou:  0.6521956991103741\n","iou:  0.6089056751346362\n","iou:  0.4372208915721352\n","000000000042\n","iou:  0.6158443856911584\n","000000000043\n","iou:  0.6413587794917571\n","000000000044\n","iou:  0.3978667860668809\n","000000000045\n","iou:  0.6428946332049307\n","iou:  0.5605346097947828\n","000000000046\n","iou:  0.41163219115269006\n","000000000047\n","iou:  0.6680609967526344\n","iou:  0.42989462013197\n","iou:  0.5700762646329474\n","iou:  0.5599095618868455\n","000000000048\n","iou:  0.4383751000001336\n","000000000049\n","iou:  0.6369382597188236\n","000000000050\n","iou:  0.5943937962787591\n","000000000051\n","iou:  0.6101468229876948\n","000000000052\n","iou:  0.6756956639700654\n","000000000053\n","iou:  0.5903657736351579\n","000000000054\n","iou:  0.6861820903900123\n","000000000055\n","iou:  0.5878116078954221\n","000000000056\n","iou:  0.567757505398603\n","000000000057\n","iou:  0.386671563566322\n","iou:  0.5921172119361239\n","iou:  0.6513053296873607\n","iou:  0.4012391073974548\n","000000000058\n","iou:  0.6163158166385988\n","000000000059\n","iou:  0.6810154002863325\n","000000000060\n","iou:  0.38262937713986317\n","iou:  0.5993120697561277\n","iou:  0.6185426506255673\n","iou:  0.49054606978750165\n","000000000061\n","iou:  0.6081734352129459\n","000000000062\n","iou:  0.5271991450741877\n","000000000063\n","iou:  0.6237734170419236\n","000000000064\n","iou:  0.5926680498039392\n","000000000065\n","iou:  0.5724034590584588\n","000000000066\n","iou:  0.45079457787546856\n","iou:  0.22094731589404007\n","iou:  0.17409031197126593\n","iou:  0.5401899162692271\n","000000000067\n","iou:  0.607275617285151\n","000000000068\n","iou:  0.571187991718998\n","000000000069\n","iou:  0.6266740149688375\n","000000000070\n","iou:  0.4554671405397039\n","iou:  0.6181889531906439\n","iou:  0.6370291533596277\n","iou:  0.4279187463993767\n","000000000071\n","iou:  0.5387302932121187\n","000000000072\n","iou:  0.6326681969652292\n","000000000073\n","iou:  0.6900420529989909\n","iou:  0.6736033055565136\n","000000000074\n","iou:  0.6312228315580735\n","iou:  0.27629677599186964\n","iou:  0.290775260445595\n","iou:  0.6194341526423752\n","000000000075\n","iou:  0.590625929511761\n","000000000076\n","iou:  0.3237062210159518\n","iou:  0.6897842854060132\n","iou:  0.5803451144254934\n","iou:  0.3180529125721504\n","000000000077\n","iou:  0.639292089366999\n","000000000078\n","iou:  0.5134489674961885\n","000000000079\n","iou:  0.42109975997948584\n","iou:  0.738263454394801\n","iou:  0.6725431825855195\n","iou:  0.43511004663711395\n","000000000080\n","iou:  0.5908373546181428\n","iou:  0.7977384231883473\n","iou:  0.5776493072249255\n","iou:  0.3335554043003342\n","000000000081\n","iou:  0.44874055195393137\n","000000000082\n","iou:  0.5773391913238276\n","000000000083\n","iou:  0.5548780553840273\n","000000000084\n","iou:  0.32537812253682014\n","iou:  0.6013859116971468\n","iou:  0.6649016211074658\n","iou:  0.3730001077693576\n","000000000085\n","iou:  0.523820803707498\n","iou:  0.6102988856242293\n","iou:  0.6384007399809839\n","iou:  0.35996112547202347\n","000000000086\n","iou:  0.48046350042016356\n","000000000087\n","iou:  0.27259243706959146\n","iou:  0.6432662380953343\n","iou:  0.6747470420224271\n","iou:  0.2260681293905338\n","000000000088\n","iou:  0.5128588855911117\n","iou:  0.41511799879294686\n","iou:  0.32254476069541066\n","iou:  0.7141555321695839\n","000000000089\n","iou:  0.7073787960324401\n","iou:  0.25347059641015857\n","iou:  0.4515279379615811\n","iou:  0.6269455047995495\n","000000000090\n","iou:  0.2253912545288027\n","iou:  0.7395926555048541\n","iou:  0.6625068514699226\n","iou:  0.11998297105670204\n","000000000091\n","iou:  0.5185184758327707\n","iou:  0.4843476924949552\n","iou:  0.34346050287369856\n","iou:  0.6804163416081301\n","000000000092\n","iou:  0.606431487311282\n","000000000093\n","iou:  0.8547707144866039\n","000000000094\n","iou:  0.5235609924422853\n","iou:  0.2769646293456896\n","iou:  0.4174606362913824\n","iou:  0.5389698516141289\n","000000000095\n","iou:  0.5807743081735401\n","000000000096\n","iou:  0.6531085187640935\n","iou:  0.31148963100413457\n","iou:  0.3917841690792622\n","iou:  0.6172440896253251\n","000000000097\n","iou:  0.5815389864881559\n","iou:  0.4913274633198319\n","000000000098\n","iou:  0.660267503770407\n","000000000128\n","iou:  0.5934396302190794\n","iou:  0.47300878241842403\n","iou:  0.2745395761200342\n","iou:  0.5038465545284786\n","iou:  0.5383984589035732\n","iou:  0.17614766455844566\n","iou:  0.5548878624215604\n","iou:  0.33606225776774973\n","iou:  0.5487179892731608\n","000000000131\n","iou:  0.46902499771417827\n","000000000132\n","iou:  0.4386091641848167\n","000000000133\n","iou:  0.5835231312533622\n","000000000134\n","iou:  0.42440100307843515\n","iou:  0.5660849229504531\n","iou:  0.5914475376397865\n","iou:  0.39795418193459725\n","000000000135\n","iou:  0.27861288691110114\n","000000000136\n","iou:  0.4298435233240637\n","000000000137\n","iou:  0.720523187668888\n","000000000138\n","iou:  0.479487108211054\n","000000000139\n","iou:  0.7149586175872793\n","000000000140\n","iou:  0.3837952855287079\n","iou:  0.8233085701419337\n","iou:  0.1386421455127189\n","iou:  0.7326506125224431\n","iou:  0.390860959935671\n","iou:  0.4126051306970404\n","iou:  0.4352450921114597\n","iou:  0.14435909102846786\n","iou:  0.8295351125202725\n","000000000142\n","iou:  0.547314662956436\n","000000000143\n","iou:  0.3400983023350317\n","iou:  0.3197383248925587\n","iou:  0.33120448734554403\n","iou:  0.1899081391739784\n","iou:  0.6931911556760082\n","iou:  0.40512678310975336\n","iou:  0.2966424032853305\n","000000000144\n","iou:  0.502086167760443\n","iou:  0.10251887260433902\n","iou:  0.10408886850358695\n","iou:  0.3934287495024662\n","iou:  0.2020461933386223\n","iou:  0.3329676173911825\n","iou:  0.25329301133665416\n","iou:  0.3556762984841069\n","000000000145\n","iou:  0.2556024763388694\n","iou:  0.533637160277212\n","iou:  0.38079630917912993\n","iou:  0.21683932870554856\n","000000000146\n","iou:  0.7073666994188054\n","000000000147\n","iou:  0.5379083437217516\n","iou:  0.2659581935242902\n","iou:  0.2553452488600367\n","iou:  0.5202255838919634\n","000000000148\n","iou:  0.6091513409676338\n","iou:  0.38565092835037906\n","iou:  0.395154051307029\n","iou:  0.49410522902913956\n","iou:  0.23660425037875402\n","iou:  0.5636233840740489\n","iou:  0.49897178964600525\n","iou:  0.6743004663579673\n","iou:  0.27950976899904156\n"]}],"source":["# Visualize New Dataset Images Results\n","!python visualization/plot_coco.py --gt-anno data/coco/annotations/person_keypoints_ads_project_all_images.json --image-path data/coco/images/ads_project_all_images/ --prediction output/coco/pose_hrnet/ADS_COCO_ALL_IMAGES_w48_384x288_adam_lr1e-3/results/keypoints_ads_project_all_images_results_0.json --save-path visualization/results/ADS_COCO_ALL_IMAGES/"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":16271,"status":"ok","timestamp":1636028728604,"user":{"displayName":"David Miguel Barles Dela Cruz","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"16734584818421110252"},"user_tz":-660},"id":"xooSfGpyodsH","outputId":"91a44671-6c6e-42a7-ab2f-b4860cab6dd8"},"outputs":[{"name":"stdout","output_type":"stream","text":["loading annotations into memory...\n","Done (t=0.00s)\n","creating index...\n","index created!\n","Loading and preparing results...\n","DONE (t=0.00s)\n","creating index...\n","index created!\n","000000000104\n","iou:  0.5316990532723073\n","000000000106\n","iou:  0.758629532820842\n","iou:  0.5213159093686939\n","iou:  0.39759393386553904\n","iou:  0.7910915409583306\n","000000000107\n","iou:  0.682228489541316\n","000000000108\n","iou:  0.5106494828731658\n","000000000109\n","iou:  0.6425266129148701\n","000000000110\n","iou:  0.3663992979896688\n","iou:  0.4612371599445909\n","000000000111\n","iou:  0.4985344064099452\n","iou:  0.6948592578324186\n","iou:  0.7200718996663573\n","iou:  0.34068858893685444\n","000000000145\n","iou:  0.5545909757164942\n","iou:  0.6968318154960895\n","iou:  0.7240695111518431\n","iou:  0.5978330329578441\n","000000000146\n","iou:  0.41330733769977873\n","iou:  0.44650379762894116\n","000000000147\n","iou:  0.7701755193181079\n","000000000148\n","iou:  0.6090325085715724\n","000000000149\n","iou:  0.589289244261986\n","000000000150\n","iou:  0.28396806037578626\n","iou:  0.3545937818042297\n","iou:  0.10024270212671335\n","iou:  0.660418166658918\n","iou:  0.4907261420892149\n","iou:  0.5029561783252466\n","iou:  0.17816947340779576\n","iou:  0.5013615644653134\n","iou:  0.5890923693740812\n","iou:  0.4701688003802088\n","iou:  0.6649222188194169\n","iou:  0.11320847923980627\n","iou:  0.6452097208207878\n","iou:  0.3775226613797968\n","iou:  0.33545062934125386\n","iou:  0.1523598448359372\n","iou:  0.6123752907839976\n","iou:  0.33898000386474425\n","iou:  0.5675711443545132\n","000000000151\n","iou:  0.5091274688014372\n","000000000152\n","iou:  0.4729413027381472\n","000000000153\n","iou:  0.5230691987862691\n","000000000154\n","iou:  0.5422657700037744\n","000000000155\n","iou:  0.5208546320666451\n","iou:  0.5800948429645187\n","000000000156\n","iou:  0.7424743726796535\n","000000000157\n","iou:  0.6451130286360326\n","000000000158\n","iou:  0.6313086242481852\n","000000000159\n","iou:  0.6051073826443852\n","000000000160\n","iou:  0.6752387091281546\n"]}],"source":["# Visualize Front Images Results\n","!python visualization/plot_coco.py --gt-anno data/coco/annotations/person_keypoints_ads_project_front.json --image-path data/coco/images/ads_project_front/ --prediction output/coco/pose_hrnet/ADS_COCO_FRONT_IMAGES_w48_384x288_adam_lr1e-3/results/keypoints_ads_project_front_results_0.json --save-path visualization/results/ADS_COCO_FRONT/"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":12031,"status":"ok","timestamp":1636028750795,"user":{"displayName":"David Miguel Barles Dela Cruz","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"16734584818421110252"},"user_tz":-660},"id":"7ayiv0n_x9fh","outputId":"9f358253-65a3-4e5a-b6e1-2afa49ab698a"},"outputs":[{"name":"stdout","output_type":"stream","text":["loading annotations into memory...\n","Done (t=0.19s)\n","creating index...\n","index created!\n","Loading and preparing results...\n","DONE (t=0.00s)\n","creating index...\n","index created!\n","000000000012\n","iou:  0.5647338551283866\n","000000000013\n","iou:  0.5450407548801589\n","000000000014\n","iou:  0.6237212645138912\n","000000000015\n","iou:  0.4239833760750847\n","000000000016\n","iou:  0.5270144675119834\n","000000000017\n","iou:  0.603611206723267\n","000000000022\n","iou:  0.5259042841873934\n","000000000023\n","iou:  0.5819406940398212\n","000000000024\n","iou:  0.5889897108424135\n","000000000025\n","iou:  0.5612836415411933\n","000000000099\n","iou:  0.4010102551669768\n","000000000100\n","iou:  0.3981339705093244\n","000000000102\n","iou:  0.4956582682923179\n","000000000103\n","iou:  0.5231068352361155\n","iou:  0.11536121988591076\n","iou:  0.1583933509451981\n","iou:  0.4311558150535748\n","000000000105\n","iou:  0.560726986064864\n"]}],"source":["# Visualize Back Images Results\n","!python visualization/plot_coco.py --gt-anno data/coco/annotations/person_keypoints_ads_project_back.json --image-path data/coco/images/ads_project_back/ --prediction output/coco/pose_hrnet/ADS_COCO_BACK_IMAGES_w48_384x288_adam_lr1e-3/results/keypoints_ads_project_back_results_0.json --save-path visualization/results/ADS_COCO_BACK/"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":12610,"status":"ok","timestamp":1636028763392,"user":{"displayName":"David Miguel Barles Dela Cruz","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"16734584818421110252"},"user_tz":-660},"id":"539xIpR8x9Lb","outputId":"784d6931-f15e-4664-e696-cb31c201e570"},"outputs":[{"name":"stdout","output_type":"stream","text":["loading annotations into memory...\n","Done (t=0.35s)\n","creating index...\n","index created!\n","Loading and preparing results...\n","DONE (t=0.00s)\n","creating index...\n","index created!\n","000000000112\n","iou:  0.5045003728052013\n","000000000113\n","iou:  0.5748038537297122\n","000000000114\n","iou:  0.489346799443775\n","000000000115\n","iou:  0.5472850184336723\n","000000000116\n","iou:  0.30277450864346384\n","000000000117\n","iou:  0.2365476417541558\n","iou:  0.5154046033458728\n","iou:  0.5865381434020959\n","iou:  0.46116925474901155\n","iou:  0.7380987008851078\n","iou:  0.3642233220467757\n","iou:  0.38728039085797905\n","iou:  0.33283654881781305\n","iou:  0.2066284835227506\n","000000000118\n","iou:  0.5922331412009285\n","000000000119\n","iou:  0.5620067710959027\n","iou:  0.13314030747841932\n","iou:  0.46987598787380197\n","000000000120\n","iou:  0.4945258604146594\n","000000000121\n","iou:  0.26973685393677044\n","iou:  0.4941337680840415\n","iou:  0.4634677824045133\n","iou:  0.22462611259360052\n","000000000122\n","iou:  0.49387401930150915\n","000000000123\n","iou:  0.45189967174544404\n","000000000125\n","iou:  0.3299777032574752\n","000000000126\n","iou:  0.5551656126783462\n","000000000137\n","iou:  0.6482921164626028\n"]}],"source":["# Visualize Side Images Results\n","!python visualization/plot_coco.py --gt-anno data/coco/annotations/person_keypoints_ads_project_side.json --image-path data/coco/images/ads_project_side/ --prediction output/coco/pose_hrnet/ADS_COCO_SIDE_IMAGES_w48_384x288_adam_lr1e-3/results/keypoints_ads_project_side_results_0.json --save-path visualization/results/ADS_COCO_SIDE/"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":43873,"status":"ok","timestamp":1636029015606,"user":{"displayName":"David Miguel Barles Dela Cruz","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"16734584818421110252"},"user_tz":-660},"id":"d2nqZKzb5-kH","outputId":"3bb69d9a-f05b-41c5-87f3-839ae8a8aa90"},"outputs":[{"name":"stdout","output_type":"stream","text":["loading annotations into memory...\n","Done (t=0.32s)\n","creating index...\n","index created!\n","Loading and preparing results...\n","DONE (t=0.00s)\n","creating index...\n","index created!\n","000000000161\n","iou:  0.6844879458843433\n","iou:  0.36843494966585\n","iou:  0.39375085453969605\n","iou:  0.5609311021211298\n","000000000162\n","iou:  0.3744536936008439\n","iou:  0.4821181465502488\n","iou:  0.6186056244718955\n","iou:  0.356565361852692\n","000000000163\n","iou:  0.38134410544535713\n","iou:  0.6349790970260182\n","iou:  0.1920013496207696\n","iou:  0.5754236361131345\n","iou:  0.40219408815922447\n","iou:  0.3530688082831453\n","iou:  0.3819793839131127\n","iou:  0.2522130145976671\n","iou:  0.46217775293843283\n","000000000164\n","iou:  0.37265774364674\n","iou:  0.5052620599530686\n","iou:  0.523310578280502\n","iou:  0.32599744334200254\n","000000000165\n","iou:  0.6770663437753717\n","iou:  0.6211836488941078\n","iou:  0.750509474752199\n","iou:  0.39696836956726206\n","000000000166\n","iou:  0.4727076673635403\n","iou:  0.4287806158954361\n","iou:  0.5229232430931813\n","iou:  0.3361517745024058\n","000000000167\n","iou:  0.5115897211462358\n","iou:  0.235328900067923\n","iou:  0.507739806654821\n","iou:  0.3949406048008128\n","000000000168\n","iou:  0.38979430792741643\n","iou:  0.7028752167727521\n","iou:  0.8044031158067443\n","iou:  0.34901882824437525\n","000000000169\n","iou:  0.4343997490822066\n","iou:  0.629286738502544\n","iou:  0.6425059204876727\n","iou:  0.42354118159644527\n","000000000170\n","iou:  0.3805605613958617\n","iou:  0.7135505437675946\n","iou:  0.6529433970017225\n","iou:  0.4188814426169479\n","000000000171\n","iou:  0.8399659468524473\n","iou:  0.32459123854823474\n","iou:  0.4765161753846776\n","iou:  0.6002362173999746\n","000000000172\n","iou:  0.6569220574794516\n","iou:  0.2690085042804942\n","iou:  0.34860954205226863\n","iou:  0.5852713165352752\n","000000000173\n","iou:  0.5341444908179681\n","iou:  0.3809275791541087\n","iou:  0.48251984293573474\n","iou:  0.5779266367505802\n","000000000174\n","iou:  0.7324708343066112\n","iou:  0.34046355786199384\n","iou:  0.719677900852741\n","iou:  0.49880480678433303\n","000000000175\n","iou:  0.8426666653244509\n","iou:  0.41587459783966185\n","iou:  0.581626774084452\n","iou:  0.5691042143582213\n"]}],"source":["# Visualize Multilpe People Images Results\n","!python visualization/plot_coco.py --gt-anno data/coco/annotations/person_keypoints_ads_project_multiple_people.json --image-path data/coco/images/ads_project_multiple_people/ --prediction output/coco/pose_hrnet/ADS_COCO_MULTIPLE_PEOPLE_IMAGES_w48_384x288_adam_lr1e-3/results/keypoints_ads_project_multiple_people_results_0.json --save-path visualization/results/ADS_COCO_MULTIPLE_PEOPLE/"]},{"cell_type":"markdown","metadata":{"id":"LeQeRXF2_xVg"},"source":["## Evaluation\n","\n","This code block shows the manual use of the COCOAPI code to obtain the evaluation results on the Person Bounding Box and Keypoint Estimation problems."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"5WZdC7YiY6cN"},"outputs":[],"source":["from pycocotools.coco import COCO\n","from pycocotools.cocoeval import COCOeval"]},{"cell_type":"markdown","metadata":{"id":"z0NjlwJAZrw1"},"source":["### Keypoint Estimation Evaluation\n","\n","This block of code illustrates the evaluation of the Output Predicted Keypoint Annotations JSON file obtained from the HRNet Model and the Ground Truth Keypoint Annotations JSON file. The results below shows the results of the HRNet-W48 model on the Multiple People Sub-dataset that we have created. This only serves as a manual illustration on the evaluation process and is already done by the evaluation script provided by the researchers."]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":1239,"status":"ok","timestamp":1636087095469,"user":{"displayName":"David DC","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjMrLKWRXXOVdd_qcD61_IIw0XnDbqJR48uWgjcVQ=s64","userId":"03757035561098753631"},"user_tz":-660},"id":"0HeBgSvq-1md","outputId":"5501b210-950d-4b90-a436-df52cafe063e"},"outputs":[{"name":"stdout","output_type":"stream","text":["Running demo for *keypoints* results.\n","loading annotations into memory...\n","Done (t=0.00s)\n","creating index...\n","index created!\n","Loading and preparing results...\n","DONE (t=1.07s)\n","creating index...\n","index created!\n","Running per image evaluation...\n","Evaluate annotation type *keypoints*\n","DONE (t=0.02s).\n","Accumulating evaluation results...\n","DONE (t=0.01s).\n"," Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets= 20 ] = 0.948\n"," Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets= 20 ] = 1.000\n"," Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets= 20 ] = 1.000\n"," Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets= 20 ] = 1.000\n"," Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets= 20 ] = 0.948\n"," Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 20 ] = 0.961\n"," Average Recall     (AR) @[ IoU=0.50      | area=   all | maxDets= 20 ] = 1.000\n"," Average Recall     (AR) @[ IoU=0.75      | area=   all | maxDets= 20 ] = 1.000\n"," Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets= 20 ] = 1.000\n"," Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets= 20 ] = 0.960\n"]}],"source":["##### Keypoint OKS Evaluation\n","annType = ['segm','bbox','keypoints']\n","annType = annType[2]      #specify type here\n","prefix = 'person_keypoints' if annType=='keypoints' else 'instances'\n","print('Running demo for *%s* results.'%(annType))\n","\n","#Initialize COC ground truth api\n","cocoGt=COCO('data/coco/annotations/person_keypoints_ads_project_multiple_people.json')\n","\n","#initialize COCO detections api\n","cocoDt=cocoGt.loadRes('output/coco/pose_hrnet/ADS_COCO_MULTIPLE_PEOPLE_IMAGES_w48_384x288_adam_lr1e-3/results/keypoints_ads_project_multiple_people_results_0.json')\n","\n","# Running evaluation\n","cocoEval = COCOeval(cocoGt,cocoDt,annType)\n","cocoEval.evaluate()\n","cocoEval.accumulate()\n","cocoEval.summarize()"]},{"cell_type":"markdown","metadata":{"id":"_JfMxNPaZtmR"},"source":["### Person Bounding Box Evaluation\n","\n","This block of code illustrates the evaluation of the Predicted Person Bounding Box JSON file provided by the researchers and the Ground Truth Person Bounding Boxes. As we can see in the results below, the person detection results stated by the researchers in the Original Research was validated wherein their Person Detection model obtained an Average Precision (AP) score of 56%."]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":23091,"status":"ok","timestamp":1636087271814,"user":{"displayName":"David DC","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjMrLKWRXXOVdd_qcD61_IIw0XnDbqJR48uWgjcVQ=s64","userId":"03757035561098753631"},"user_tz":-660},"id":"ILL3XR5fjxjv","outputId":"d57d65e3-4ea7-41c0-bb6c-db6ef6620011"},"outputs":[{"name":"stdout","output_type":"stream","text":["Running demo for *bbox* results.\n","loading annotations into memory...\n","Done (t=0.27s)\n","creating index...\n","index created!\n","Loading and preparing results...\n","DONE (t=0.96s)\n","creating index...\n","index created!\n","Running per image evaluation...\n","Evaluate annotation type *bbox*\n","DONE (t=19.85s).\n","Accumulating evaluation results...\n","DONE (t=1.60s).\n"," Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.564\n"," Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.834\n"," Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.625\n"," Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.380\n"," Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.633\n"," Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.745\n"," Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.194\n"," Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.572\n"," Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.688\n"," Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.519\n"," Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.759\n"," Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.858\n"]}],"source":["##### Person Bounding Box Evaluation\n","annType = ['segm','bbox','keypoints']\n","annType = annType[1]      #specify type here\n","prefix = 'person_keypoints' if annType=='keypoints' else 'instances'\n","print('Running demo for *%s* results.'%(annType))\n","\n","#Initialize COCO ground truth api\n","cocoGt=COCO('data/coco/annotations/person_keypoints_val2017.json')\n","\n","#initialize COCO detections api\n","cocoDt=cocoGt.loadRes('data/coco/person_detection_results/COCO_val2017_detections_AP_H_56_person.json')\n","\n","# Running evaluation\n","cocoEval = COCOeval(cocoGt,cocoDt,annType)\n","cocoEval.evaluate()\n","cocoEval.accumulate()\n","cocoEval.summarize()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Hkjmlp5hkw5e"},"outputs":[],"source":[]},{"cell_type":"markdown","metadata":{"id":"mX8g0ow-z1de"},"source":["## Exploratory Data Analysis\n","\n","This section contains a simple exploratory data analysis between the Ground Truch Keypoint Annotation JSON File for the COCO 2017 Validation Dataset and the Predicted Person Bounding Boxes JSON file provided by the researchers."]},{"cell_type":"code","execution_count":5,"metadata":{"executionInfo":{"elapsed":3419,"status":"ok","timestamp":1636111135987,"user":{"displayName":"David Miguel Barles Dela Cruz","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"16734584818421110252"},"user_tz":-660},"id":"9bWxCCzyTU2d"},"outputs":[],"source":["with open('data/coco/annotations/person_keypoints_val2017.json') as fp:\n","  gt_anno = json.load(fp)\n","\n","with open('data/coco/annotations/person_keypoints_ads_project_all_images.json') as fp:\n","  gt_anno_ads = json.load(fp)\n","\n","with open('data/coco/person_detection_results/COCO_val2017_detections_AP_H_56_person.json') as fp:\n","  pred_anno = json.load(fp)"]},{"cell_type":"markdown","metadata":{"id":"95WvUGjOcpw8"},"source":["Observing the number of Person Bounding Boxes found in the Ground Truth JSON file and the Predicted Bounding Box JSON file, we can see that the Ground Truth JSON file only contained 11,004 bounding boxes while the Predicted Bounding Boxes obtained from the Person Detection model of the researchers created 104,125 bounding boxes. This explains why the evaluation on the 2017 COCO Validation Dataset using the Predicted Bounding Box JSON File took almost 2 hours to execute while the use of the Ground Truth Bounding Boxes only took an estimate of 10 minutes."]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":3,"status":"ok","timestamp":1636100480676,"user":{"displayName":"David Miguel Barles Dela Cruz","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"16734584818421110252"},"user_tz":-660},"id":"ZZ041m9pTx75","outputId":"aabad0da-9566-4168-cff7-06891882ce94"},"outputs":[{"name":"stdout","output_type":"stream","text":["Ground Truth Annotation No.: 11004\n","Predicted Persoun Bounding Box No.: 104125\n"]}],"source":["print(\"Ground Truth Annotation No.:\", len(gt_anno['annotations']))\n","print(\"Predicted Persoun Bounding Box No.:\", len(pred_anno))"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":58947,"status":"ok","timestamp":1636100540017,"user":{"displayName":"David Miguel Barles Dela Cruz","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"16734584818421110252"},"user_tz":-660},"id":"zq2WjO2SW-d4","outputId":"577699bf-7346-4e8f-8fc1-d1d0f1f23079"},"outputs":[{"name":"stdout","output_type":"stream","text":["Number of Images (2017 COCO Validation Set):\n","5000\n"]}],"source":["print(\"Number of Images (2017 COCO Validation Set):\")\n","!ls data/coco/images/val2017/ | wc -l"]},{"cell_type":"markdown","metadata":{"id":"awkC4g9oMDV9"},"source":["### Number of Images without Person Bounding Box"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":427,"status":"ok","timestamp":1636100738552,"user":{"displayName":"David Miguel Barles Dela Cruz","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"16734584818421110252"},"user_tz":-660},"id":"39CgPaO_MnZP","outputId":"09221f32-382d-4556-bc84-6f964102f89c"},"outputs":[{"name":"stdout","output_type":"stream","text":["Number of Images without Person Bounding Box:\n"]},{"data":{"text/plain":["2307"]},"execution_count":12,"metadata":{},"output_type":"execute_result"}],"source":["print(\"Number of Images without Person Bounding Box:\")\n","5000-len(set([annotation['image_id'] for annotation in gt_anno['annotations']]))"]},{"cell_type":"markdown","metadata":{"id":"dg7LepQeNK9A"},"source":["### Distribution of different Sized Objects\n","\n"]},{"cell_type":"code","execution_count":6,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":289},"executionInfo":{"elapsed":8,"status":"ok","timestamp":1636111136980,"user":{"displayName":"David Miguel Barles Dela Cruz","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"16734584818421110252"},"user_tz":-660},"id":"_xl6E6ekNOiK","outputId":"d5c9f87d-8376-446a-c4bb-c95e941e9e4e"},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>area</th>\n","      <th>bbox</th>\n","      <th>category_id</th>\n","      <th>id</th>\n","      <th>image_id</th>\n","      <th>iscrowd</th>\n","      <th>keypoints</th>\n","      <th>num_keypoints</th>\n","      <th>segmentation</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>90545.231548</td>\n","      <td>[0, 41.7971391105201, 145.53486997635903, 622....</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>34</td>\n","      <td>0</td>\n","      <td>[22.163522458628943, 124.67543698286049, 2, 28...</td>\n","      <td>9</td>\n","      <td>[[]]</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>93271.730000</td>\n","      <td>[62.6281323811049, 6.45143951561248, 212.55233...</td>\n","      <td>1</td>\n","      <td>3</td>\n","      <td>144</td>\n","      <td>0</td>\n","      <td>[148.10544883907122, 51.08652554843877, 2, 146...</td>\n","      <td>16</td>\n","      <td>[[]]</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>32724.111111</td>\n","      <td>[10.1666666666667, 32.1380208333333, 157.83333...</td>\n","      <td>1</td>\n","      <td>4</td>\n","      <td>61</td>\n","      <td>0</td>\n","      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 81.16666666666667,...</td>\n","      <td>14</td>\n","      <td>[[]]</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>26987.470000</td>\n","      <td>[128.857142857143, 0.0946428571428571, 138.742...</td>\n","      <td>1</td>\n","      <td>5</td>\n","      <td>88</td>\n","      <td>0</td>\n","      <td>[178, 74.15178571428571, 2, 196.28571428571428...</td>\n","      <td>9</td>\n","      <td>[[]]</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>19168.135314</td>\n","      <td>[0, 171.204248564945, 54.7055702917772, 350.38...</td>\n","      <td>1</td>\n","      <td>6</td>\n","      <td>5</td>\n","      <td>0</td>\n","      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n","      <td>5</td>\n","      <td>[[]]</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["           area  ... segmentation\n","0  90545.231548  ...         [[]]\n","1  93271.730000  ...         [[]]\n","2  32724.111111  ...         [[]]\n","3  26987.470000  ...         [[]]\n","4  19168.135314  ...         [[]]\n","\n","[5 rows x 9 columns]"]},"execution_count":6,"metadata":{},"output_type":"execute_result"}],"source":["# COCO 2017 Validation Annotations\n","gt_anno_df = pd.DataFrame(gt_anno['annotations'])\n","gt_anno_df.head()\n","\n","# New Dataset Annotations\n","gt_anno_ads_df = pd.DataFrame(gt_anno_ads['annotations'])\n","gt_anno_ads_df.head()"]},{"cell_type":"code","execution_count":8,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":468,"status":"ok","timestamp":1636111141870,"user":{"displayName":"David Miguel Barles Dela Cruz","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"16734584818421110252"},"user_tz":-660},"id":"vsTsoWpCNtW9","outputId":"21fa50f5-baf1-4673-e3ae-66c01b32cc21"},"outputs":[{"name":"stdout","output_type":"stream","text":["Distribution of Object Scales (2017 COCO Validation Dataset):\n","Small Scale Objects: 4336\n","Medium Scale Objects: 3800\n","Large Scale Objects: 2868\n","\n","Distribution of Object Scales (ADS Project - Full Dataset):\n","Small Scale Objects: 0\n","Medium Scale Objects: 7\n","Large Scale Objects: 151\n"]}],"source":["print(\"Distribution of Object Scales (2017 COCO Validation Dataset):\")\n","print(\"Small Scale Objects: {}\".format(len(gt_anno_df[gt_anno_df['area'] < 1024])))\n","print(\"Medium Scale Objects: {}\".format(len(gt_anno_df[(gt_anno_df['area'] >= 1024) & (gt_anno_df['area'] < 9216)])))\n","print(\"Large Scale Objects: {}\".format(len(gt_anno_df[gt_anno_df['area'] >= 9216])))\n","\n","print(\"\\nDistribution of Object Scales (ADS Project - Full Dataset):\")\n","print(\"Small Scale Objects: {}\".format(len(gt_anno_ads_df[gt_anno_ads_df['area'] < 1024])))\n","print(\"Medium Scale Objects: {}\".format(len(gt_anno_ads_df[(gt_anno_ads_df['area'] >= 1024) & (gt_anno_ads_df['area'] < 9216)])))\n","print(\"Large Scale Objects: {}\".format(len(gt_anno_ads_df[gt_anno_ads_df['area'] >= 9216])))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"HehoKOI70pAr"},"outputs":[],"source":[]}],"metadata":{"accelerator":"GPU","colab":{"collapsed_sections":["ZVfFMstwI9YY","DyJZtTd2da3-","u0nR4-h_deHv","k-y8o1aWnj_z","Ig2fW05jsfvD","02XLo4dqsiyJ","IZTSXcT0wsGf","_5vdGHELwt65","wHfc2kIY2ME-","HKoXyPBn6uUq","LeQeRXF2_xVg","z0NjlwJAZrw1","_JfMxNPaZtmR"],"name":"ADS-GroupG.ipynb","provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}
